"""
Mamba-Adaptiveæ‰«æè·¯å¾„æ¨¡å‹è®­ç»ƒè„šæœ¬
ç»“åˆ Mamba + AdaptiveNN Focusæœºåˆ¶
"""
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import json
import numpy as np
from datetime import datetime
from tqdm import tqdm
from pathlib import Path

from config_mamba_adaptive import MambaAdaptiveConfig
from data.dataset import create_dataloaders
from models.mamba_adaptive_scanpath import MambaAdaptiveScanpath
import math


def compute_teacher_forcing_ratio(epoch, step_idx=None):
    """
    ä¿®å¤ç‰ˆTeacher Forcingç­–ç•¥ï¼ˆæ”¹å–„åºåˆ—å¯¹é½ï¼Œä¿®å¤RECä¸º0é—®é¢˜ï¼‰
    
    å…³é”®ä¿®å¤ï¼š
    1. ä¿æŒè¾ƒé«˜çš„Teacher Forcingæ¯”ä¾‹ï¼Œç¡®ä¿æ¨¡å‹å­¦ä¹ çœŸå®åºåˆ—
    2. æ›´æ…¢çš„è¡°å‡é€Ÿåº¦ï¼Œé¿å…è®­ç»ƒä¸ç¨³å®š
    3. å‰å‡ æ­¥ä¿æŒé«˜TFï¼Œç¡®ä¿åºåˆ—èµ·å§‹æ­£ç¡®

    Args:
        epoch: å½“å‰è®­ç»ƒè½®æ¬¡
        step_idx: å½“å‰åºåˆ—ä¸­çš„æ­¥éª¤ç´¢å¼•ï¼ˆ0-29ï¼‰ï¼Œç”¨äºå‰å‡ æ­¥ä¿æŒé«˜TF
    """
    initial_ratio = 0.95  # æé«˜åˆå§‹æ¯”ä¾‹ï¼Œç¡®ä¿æ—©æœŸè®­ç»ƒç¨³å®š
    final_ratio = 0.5  # æé«˜æœ€ç»ˆæ¯”ä¾‹ï¼Œå‡å°‘è®­ç»ƒå’Œæ¨ç†å·®å¼‚
    decay_epochs = 50  # 50 epoch

    # çº¿æ€§è¡°å‡ï¼ˆæ›´ç¨³å®šï¼‰ï¼šratio = initial - (initial - final) * (epoch / decay_epochs)
    base_ratio = initial_ratio - (initial_ratio - final_ratio) * min(epoch / decay_epochs, 1.0)
    base_ratio = max(base_ratio, final_ratio)

    # å‰å‡ æ­¥å¹³æ»‘è¡°å‡ï¼ˆæ›´ä¿å®ˆçš„ç­–ç•¥ï¼Œç¡®ä¿åºåˆ—å¯¹é½ï¼‰
    if step_idx is not None:
        if step_idx < 3:
            # å‰3æ­¥ï¼šé¢å¤–+0.05ï¼Œç¡®ä¿èµ·å§‹ç¨³å®š
            return min(base_ratio + 0.05, 1.0)
        elif step_idx < 6:
            # 3-6æ­¥ï¼šé¢å¤–+0.03
            return min(base_ratio + 0.03, 0.98)
        elif step_idx < 10:
            # 6-10æ­¥ï¼šé¢å¤–+0.01
            return min(base_ratio + 0.01, 0.95)

    return base_ratio


def compute_spatial_coverage_loss(pred_scanpaths):
    """åˆå¹¶è¦†ç›–èŒƒå›´ã€å¤šæ ·æ€§å’Œä¸­å¿ƒèšé›†æƒ©ç½šï¼ˆæ”¹è¿›ç‰ˆï¼šæé«˜Yæ–¹å‘è¦†ç›–ï¼‰"""
    # è¦†ç›–èŒƒå›´
    pred_min = pred_scanpaths.min(dim=1)[0]
    pred_max = pred_scanpaths.max(dim=1)[0]
    pred_range = pred_max - pred_min

    # æé«˜è¦†ç›–ç›®æ ‡ï¼šXæ–¹å‘0.5ï¼ŒYæ–¹å‘0.5ï¼ˆä¹‹å‰æ˜¯0.3å’Œ0.25ï¼‰
    coverage_x = torch.mean(((0.5 - pred_range[:, 0]).clamp(min=0.0)) ** 2)
    coverage_y = torch.mean(((0.5 - pred_range[:, 1]).clamp(min=0.0)) ** 2)

    # å¤šæ ·æ€§
    pred_mean = pred_scanpaths.mean(dim=1)
    pred_var = ((pred_scanpaths - pred_mean.unsqueeze(1)) ** 2).mean(dim=1)

    diversity_x = torch.mean(((0.015 - pred_var[:, 0]).clamp(min=0.0)) ** 2)
    diversity_y = torch.mean(((0.025 - pred_var[:, 1]).clamp(min=0.0)) ** 2)

    # Yæ–¹å‘ä¸­å¿ƒèšé›†æƒ©ç½šï¼ˆä¿®å¤ï¼šæƒ©ç½šåç¦»0.5çš„ä»»ä½•æ–¹å‘ï¼‰
    y_center_dist = torch.abs(pred_mean[:, 1] - 0.5)
    # å…è®¸Â±0.1çš„åå·®ï¼ˆæ”¾å®½é™åˆ¶ï¼‰ï¼Œè¶…å‡ºåˆ™æƒ©ç½š
    y_bias_penalty = torch.mean((y_center_dist - 0.1).clamp(min=0.0) ** 2)

    # å†…éƒ¨åŠ æƒç»„åˆï¼ˆé™ä½y_bias_penaltyæƒé‡ï¼Œç»™Yæ–¹å‘æ›´å¤šè‡ªç”±ï¼‰
    return coverage_x + 3.0 * coverage_y + diversity_x + 5.0 * diversity_y + 5.0 * y_bias_penalty


def compute_trajectory_smoothness_loss(pred_scanpaths, true_scanpaths):
    """åˆå¹¶æ­¥é•¿ã€è·³è·ƒå’ŒåŠ é€Ÿåº¦çº¦æŸï¼ˆä¿®å¤ï¼šç§»é™¤è¿‡åº¦çš„è·³è·ƒæƒ©ç½šï¼‰"""
    pred_diffs = pred_scanpaths[:, 1:] - pred_scanpaths[:, :-1]
    true_diffs = true_scanpaths[:, 1:] - true_scanpaths[:, :-1]

    pred_steps = torch.norm(pred_diffs, p=2, dim=-1)
    true_steps = torch.norm(true_diffs, p=2, dim=-1)

    # æ­¥é•¿åŒ¹é…
    step_loss = F.mse_loss(pred_steps, true_steps)

    # è·³è·ƒæƒ©ç½šï¼ˆä¿®å¤ï¼šæé«˜é˜ˆå€¼åˆ°0.5ï¼Œå…è®¸æ›´å¤§çš„æ­¥é•¿ï¼‰
    # ä¹‹å‰0.2å¤ªå°ï¼Œå¯¼è‡´è·¯å¾„ç§»åŠ¨è·ç¦»è¿‡çŸ­
    jump_loss = torch.mean((pred_steps - 0.5).clamp(min=0.0) ** 2)

    # åŠ é€Ÿåº¦çº¦æŸ
    if pred_steps.shape[1] > 1:
        pred_accel = pred_steps[:, 1:] - pred_steps[:, :-1]
        true_accel = true_steps[:, 1:] - true_steps[:, :-1]
        accel_loss = F.mse_loss(pred_accel, true_accel)
    else:
        accel_loss = torch.tensor(0.0, device=pred_scanpaths.device)

    # é™ä½jump_lossæƒé‡ï¼ˆä»0.5åˆ°0.1ï¼‰
    return step_loss + 0.1 * jump_loss + 0.3 * accel_loss


def compute_direction_consistency_loss(pred_scanpaths, true_scanpaths):
    """åˆå¹¶æ–¹å‘å˜åŒ–å’Œæ–¹å‘è¿ç»­æ€§"""
    pred_diffs = pred_scanpaths[:, 1:] - pred_scanpaths[:, :-1]
    true_diffs = true_scanpaths[:, 1:] - true_scanpaths[:, :-1]

    pred_steps = torch.norm(pred_diffs, p=2, dim=-1, keepdim=True)
    true_steps = torch.norm(true_diffs, p=2, dim=-1, keepdim=True)

    pred_directions = pred_diffs / (pred_steps + 1e-8)
    true_directions = true_diffs / (true_steps + 1e-8)

    # æ–¹å‘å˜åŒ–
    if pred_directions.shape[1] > 1:
        pred_dir_diffs = pred_directions[:, 1:] - pred_directions[:, :-1]
        true_dir_diffs = true_directions[:, 1:] - true_directions[:, :-1]
        direction_loss = F.mse_loss(
            torch.norm(pred_dir_diffs, p=2, dim=-1),
            torch.norm(true_dir_diffs, p=2, dim=-1)
        )
    else:
        direction_loss = torch.tensor(0.0, device=pred_scanpaths.device)

    # æ–¹å‘è¿ç»­æ€§
    if pred_directions.shape[1] > 0:
        pred_similarity = F.cosine_similarity(pred_directions[:, :-1], pred_directions[:, 1:], dim=-1)
        true_similarity = F.cosine_similarity(true_directions[:, :-1], true_directions[:, 1:], dim=-1)
        continuity_loss = F.mse_loss(pred_similarity, true_similarity)
    else:
        continuity_loss = torch.tensor(0.0, device=pred_scanpaths.device)

    return direction_loss + continuity_loss


def compute_sequence_alignment_loss(pred_scanpaths, true_scanpaths):
    """
    å®Œæ•´åºåˆ—å¯¹é½æŸå¤±ï¼šçº¦æŸæ‰€æœ‰30æ­¥ï¼ˆæ–¹æ¡ˆA - ç²¾ç¡®å¤åˆ¶ï¼Œä¿®å¤ç‰ˆï¼‰

    å…³é”®æ”¹è¿›ï¼š
    - çº¦æŸæ‰€æœ‰30æ­¥ï¼Œç¡®ä¿å®Œæ•´åºåˆ—å¯¹é½
    - é™ä½å†…éƒ¨æƒé‡ï¼Œé¿å…è¿‡åº¦å…³æ³¨å‰å‡ æ­¥å¯¼è‡´è·¯å¾„"å¡ä½"
    - ç›®æ ‡ï¼šè®©æ¨¡å‹å­¦ä¼š"ç²¾ç¡®å¤åˆ¶"çœŸå®è·¯å¾„çš„å®Œæ•´è½¨è¿¹
    """
    B, T, D = pred_scanpaths.shape

    # è®¡ç®—æ‰€æœ‰æ—¶é—´æ­¥çš„ç‚¹å¯¹ç‚¹è·ç¦»
    point_distances = torch.norm(pred_scanpaths - true_scanpaths, dim=-1)  # (B, T)

    # æƒé‡é…ç½®ï¼šé™ä½æƒé‡ï¼Œé¿å…è¿‡åº¦çº¦æŸï¼ˆä¹‹å‰æƒé‡å¤ªé«˜å¯¼è‡´æ¨¡å‹"å¡ä½"ï¼‰
    weights = torch.ones(T, device=pred_scanpaths.device)
    weights[:5] = 3.0  # å‰5æ­¥ï¼šæƒé‡3.0ï¼ˆä»15.0é™ä½ï¼‰
    weights[5:10] = 2.5  # 5-10æ­¥ï¼šæƒé‡2.5ï¼ˆä»10.0é™ä½ï¼‰
    weights[10:15] = 2.0  # 10-15æ­¥ï¼šæƒé‡2.0ï¼ˆä»8.0é™ä½ï¼‰
    weights[15:20] = 1.5  # 15-20æ­¥ï¼šæƒé‡1.5ï¼ˆä»6.0é™ä½ï¼‰
    weights[20:25] = 1.3  # 20-25æ­¥ï¼šæƒé‡1.3ï¼ˆä»5.0é™ä½ï¼‰
    weights[25:] = 1.2  # 25-30æ­¥ï¼šæƒé‡1.2ï¼ˆä»4.0é™ä½ï¼‰

    # è®¡ç®—æ‰€æœ‰30æ­¥çš„åŠ æƒå¹³å‡
    alignment_loss = torch.mean(point_distances * weights.unsqueeze(0))

    return alignment_loss


def compute_motion_consistency_loss(pred_scanpaths, true_scanpaths):
    """
    è¿åŠ¨ä¸€è‡´æ€§æŸå¤±ï¼ˆæ–¹æ¡ˆC-æ”¹è¿›ç‰ˆï¼‰ï¼šé€‰æ‹©æ€§çº¦æŸæ–¹å‘å’Œæ­¥é•¿

    æ”¹è¿›ï¼šåªçº¦æŸ"åˆç†"çš„è¿åŠ¨ï¼Œé¿å…è¿‡åº¦çº¦æŸå¯¼è‡´Nå½¢è·¯å¾„

    åŒ…å«ä¸¤ä¸ªéƒ¨åˆ†ï¼š
    1. æ–¹å‘ç›¸ä¼¼åº¦æŸå¤±ï¼šä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦çº¦æŸè¿åŠ¨æ–¹å‘
    2. æ­¥é•¿ç›¸ä¼¼åº¦æŸå¤±ï¼šä½¿ç”¨MSEçº¦æŸè¿åŠ¨æ­¥é•¿

    å…³é”®æ”¹è¿›ï¼š
    - åªå¯¹æ­¥é•¿åœ¨åˆç†èŒƒå›´å†…çš„è¿åŠ¨è¿›è¡Œçº¦æŸ
    - å¯¹äºè¿‡å°çš„æ­¥é•¿ï¼ˆ< 0.01ï¼‰ï¼Œä¸çº¦æŸæ–¹å‘ï¼ˆé¿å…å™ªå£°ï¼‰
    - å¯¹äºè¿‡å¤§çš„æ­¥é•¿ï¼ˆ> 0.3ï¼‰ï¼Œé™ä½çº¦æŸæƒé‡ï¼ˆå…è®¸æ¢ç´¢ï¼‰

    Args:
        pred_scanpaths: é¢„æµ‹è·¯å¾„ (B, T, 2)
        true_scanpaths: çœŸå®è·¯å¾„ (B, T, 2)

    Returns:
        motion_loss: è¿åŠ¨ä¸€è‡´æ€§æŸå¤±æ ‡é‡
    """
    # è®¡ç®—è¿åŠ¨å‘é‡ï¼ˆç›¸é‚»ç‚¹ä¹‹é—´çš„ä½ç§»ï¼‰
    pred_motions = pred_scanpaths[:, 1:] - pred_scanpaths[:, :-1]  # (B, T-1, 2)
    true_motions = true_scanpaths[:, 1:] - true_scanpaths[:, :-1]  # (B, T-1, 2)

    # è®¡ç®—æ­¥é•¿
    pred_step_lengths = torch.norm(pred_motions, p=2, dim=-1)  # (B, T-1)
    true_step_lengths = torch.norm(true_motions, p=2, dim=-1)  # (B, T-1)

    # 1. æ–¹å‘ç›¸ä¼¼åº¦æŸå¤±ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰- é€‰æ‹©æ€§çº¦æŸ
    # å½’ä¸€åŒ–è¿åŠ¨å‘é‡å¾—åˆ°æ–¹å‘
    pred_directions = F.normalize(pred_motions, p=2, dim=-1, eps=1e-8)  # (B, T-1, 2)
    true_directions = F.normalize(true_motions, p=2, dim=-1, eps=1e-8)  # (B, T-1, 2)

    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    cosine_similarity = (pred_directions * true_directions).sum(dim=-1)  # (B, T-1)

    # é€‰æ‹©æ€§çº¦æŸï¼šåªå¯¹åˆç†æ­¥é•¿çš„è¿åŠ¨çº¦æŸæ–¹å‘
    # æ­¥é•¿å¤ªå°ï¼ˆ< 0.01ï¼‰ï¼šå¯èƒ½æ˜¯å™ªå£°ï¼Œä¸çº¦æŸ
    # æ­¥é•¿å¤ªå¤§ï¼ˆ> 0.3ï¼‰ï¼šå¯èƒ½æ˜¯æ¢ç´¢æ€§è·³è·ƒï¼Œé™ä½çº¦æŸ
    step_mask = (true_step_lengths > 0.01) & (true_step_lengths < 0.3)  # (B, T-1)

    # å¯¹äºå¤§æ­¥é•¿ï¼Œä½¿ç”¨è¾ƒå°çš„æƒé‡
    large_step_mask = true_step_lengths >= 0.3
    large_step_weight = 0.3  # å¤§æ­¥é•¿çš„æ–¹å‘çº¦æŸæƒé‡é™ä½åˆ°30%

    # è®¡ç®—åŠ æƒæ–¹å‘æŸå¤±
    direction_loss_per_step = 1.0 - cosine_similarity  # (B, T-1)
    direction_loss_weighted = torch.where(
        step_mask,
        direction_loss_per_step,  # æ­£å¸¸æ­¥é•¿ï¼šå…¨æƒé‡
        torch.where(
            large_step_mask,
            direction_loss_per_step * large_step_weight,  # å¤§æ­¥é•¿ï¼šé™ä½æƒé‡
            torch.zeros_like(direction_loss_per_step)  # å°æ­¥é•¿ï¼šä¸çº¦æŸ
        )
    )
    direction_loss = direction_loss_weighted.mean()

    # 2. æ­¥é•¿ç›¸ä¼¼åº¦æŸå¤±ï¼ˆMSEï¼‰- ä½¿ç”¨ç›¸å¯¹è¯¯å·®è€Œä¸æ˜¯ç»å¯¹è¯¯å·®
    # æ”¹è¿›ï¼šä½¿ç”¨ç›¸å¯¹è¯¯å·®ï¼Œé¿å…å¯¹å¤§æ­¥é•¿è¿‡åº¦æƒ©ç½š
    # ç›¸å¯¹è¯¯å·® = |pred - true| / (true + eps)
    step_relative_error = torch.abs(pred_step_lengths - true_step_lengths) / (true_step_lengths + 1e-6)

    # åªå¯¹åˆç†æ­¥é•¿çº¦æŸ
    step_loss_per_step = step_relative_error ** 2
    step_loss_weighted = torch.where(
        step_mask,
        step_loss_per_step,
        torch.zeros_like(step_loss_per_step)
    )
    step_length_loss = step_loss_weighted.mean()

    # 3. ç»„åˆæŸå¤±ï¼ˆæ–¹å‘å’Œæ­¥é•¿åŒç­‰é‡è¦ï¼‰
    motion_loss = direction_loss + step_length_loss

    return motion_loss


# å·²ç§»é™¤ compute_batch_diversity_loss å‡½æ•°
# æ–¹æ¡ˆAï¼šä¸“æ³¨äºç²¾ç¡®å¤åˆ¶è·¯å¾„ï¼Œä¸é¼“åŠ±å¤šæ ·æ€§


def train():
    """è®­ç»ƒä¸»å‡½æ•°"""
    config = MambaAdaptiveConfig()

    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(config.log_dir, exist_ok=True)
    os.makedirs(config.checkpoint_dir, exist_ok=True)

    # åŠ è½½æ•°æ®
    print("åŠ è½½æ•°æ®...")
    train_loader, test_loader = create_dataloaders(config)
    print(f"è®­ç»ƒé›†: {len(train_loader)} batches")
    print(f"æµ‹è¯•é›†: {len(test_loader)} batches")

    # åˆ›å»ºæ¨¡å‹
    print("\nåˆ›å»ºMamba-Adaptiveæ¨¡å‹ï¼ˆç»“åˆFocusæœºåˆ¶ï¼‰...")
    model = MambaAdaptiveScanpath(config).to(config.device)
    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M")

    # ä¼˜åŒ–å™¨ï¼ˆä¼˜åŒ–ï¼šé™ä½åˆå§‹å­¦ä¹ ç‡ï¼Œå¢å¼ºæ­£åˆ™åŒ–ï¼‰
    # ä¿®å¤ï¼šé™ä½åˆå§‹å­¦ä¹ ç‡ï¼Œä»0.00012é™åˆ°0.00005ï¼Œè§£å†³è¿‡æ‹Ÿåˆ
    initial_lr = config.learning_rate * 0.4  # é™ä½åˆ°åŸæ¥çš„40%ï¼ˆ0.00012 * 0.4 = 0.000048ï¼‰
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=initial_lr,  # ä½¿ç”¨é™ä½åçš„å­¦ä¹ ç‡
        weight_decay=config.weight_decay * 1.5,  # å¢åŠ weight decayï¼šä»2e-3æé«˜åˆ°3e-3
        betas=(0.9, 0.999),  # é»˜è®¤å€¼ï¼Œä½†æ˜¾å¼æŒ‡å®š
        eps=1e-8  # é»˜è®¤å€¼ï¼Œä½†æ˜¾å¼æŒ‡å®š
    )

    # å­¦ä¹ ç‡è°ƒåº¦å™¨ - ä½¿ç”¨å¸¦warmupçš„ä½™å¼¦é€€ç«ï¼ˆæ”¹å–„è®­ç»ƒç¨³å®šæ€§ï¼‰
    # ä¼˜åŒ–ï¼šå»¶é•¿warmupé˜¶æ®µï¼Œé™ä½åˆå§‹å­¦ä¹ ç‡ï¼Œè§£å†³è¿‡æ‹Ÿåˆ
    warmup_epochs = 10  # å»¶é•¿warmupï¼šä»5ä¸ªepochå»¶é•¿åˆ°10ä¸ªepoch
    
    # ä½¿ç”¨SequentialLRç»„åˆwarmupå’Œä½™å¼¦é€€ç«
    # Warmupè°ƒåº¦å™¨ï¼ˆç”¨äºå‰warmup_epochsä¸ªepochï¼‰
    def lr_lambda_warmup(epoch):
        return (epoch + 1) / warmup_epochs  # çº¿æ€§warmup
    
    warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda_warmup)
    
    # ä½™å¼¦é€€ç«è°ƒåº¦å™¨ï¼ˆç”¨äºwarmupä¹‹åçš„epochï¼‰
    cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=config.num_epochs - warmup_epochs,
        eta_min=config.learning_rate * 0.05  # æœ€å°å­¦ä¹ ç‡ä¸ºåˆå§‹çš„5%ï¼ˆæé«˜ï¼Œé¿å…å­¦ä¹ ç‡è¿‡å°ï¼‰
    )
    
    # ç»„åˆè°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.SequentialLR(
        optimizer,
        schedulers=[warmup_scheduler, cosine_scheduler],
        milestones=[warmup_epochs]
    )

    # æ—©åœæœºåˆ¶ï¼šåŸºäºéªŒè¯ä½ç½®è¯¯å·®è€Œä¸æ˜¯æŸå¤±
    # ä¿®å¤ï¼šæ­£ç¡®è®¡ç®—patienceï¼Œè€ƒè™‘éªŒè¯é—´éš”
    best_val_position_error = float('inf')
    patience_counter = 0
    early_stopping_patience = 20  # å¢åŠ åˆ°20ï¼Œç»™æ¨¡å‹æ›´å¤šè®­ç»ƒæœºä¼šï¼ˆè€ƒè™‘éªŒè¯é—´éš”ï¼‰
    best_val_loss = float('inf')  # ä»ç„¶è®°å½•ï¼Œä½†ç”¨äºä¿å­˜æ¨¡å‹
    last_val_epoch = 0  # è®°å½•ä¸Šæ¬¡éªŒè¯çš„epoch

    # è®­ç»ƒæ—¥å¿—
    training_log = {
        'config': {
            'seq_len': config.seq_len,
            'batch_size': config.batch_size,
            'learning_rate': config.learning_rate,
            'num_epochs': config.num_epochs,
            'feature_dim': config.feature_dim,
            'd_state': config.d_state,
            'focus_patch_size': config.focus_patch_size,
        },
        'epochs': []
    }

    # è®­ç»ƒå¾ªç¯
    print("\nå¼€å§‹è®­ç»ƒ...")
    best_loss = float('inf')

    for epoch in range(1, config.num_epochs + 1):
        print(f"\n{'=' * 80}")
        print(f"Epoch {epoch}/{config.num_epochs}")
        print(f"{'=' * 80}")

        # è®­ç»ƒ
        model.train()
        epoch_loss = 0
        epoch_position_error = 0

        train_bar = tqdm(train_loader, desc="è®­ç»ƒ")
        for batch_idx, batch in enumerate(train_bar):
            images = batch['image'].to(config.device)
            true_scanpaths = batch['scanpath'].to(config.device)

            # å‰å‘ä¼ æ’­ - ä¼ é€’çœŸå®ä½ç½®ç”¨äºTeacher Forcing
            # æ”¹è¿›Teacher Forcingç­–ç•¥ï¼šæŒ‡æ•°è¡°å‡
            teacher_forcing_ratio = compute_teacher_forcing_ratio(epoch)

            # è®­ç»ƒæ—¶æ˜¾å¼è®¾ç½®enable_early_stop=Falseï¼Œç¡®ä¿è¿”å›3ä¸ªå€¼
            # use_gt_start=True ç¡®ä¿ä½¿ç”¨çœŸå®èµ·å§‹ç‚¹ï¼Œæ”¹å–„LEVæŒ‡æ ‡
            predicted_scanpaths, mus, logvars = model(
                images,
                gt_scanpaths=true_scanpaths,
                teacher_forcing_ratio=teacher_forcing_ratio,
                enable_early_stop=False,
                use_gt_start=True  # ä½¿ç”¨çœŸå®èµ·å§‹ç‚¹
            )

            # ========== ä¿®å¤ç‰ˆæŸå¤±å‡½æ•°ï¼šæ·»åŠ åƒç´ çº§è·ç¦»æŸå¤±ï¼Œä¿®å¤RECä¸º0 ==========
            # å…³é”®ä¿®å¤ï¼šRECä¸º0è¯´æ˜é¢„æµ‹è·¯å¾„å’ŒçœŸå®è·¯å¾„çš„ç‚¹å¯¹ç‚¹è·ç¦»è¿œè¶…è¿‡12åƒç´ 
            # éœ€è¦æ·»åŠ åƒç´ çº§è·ç¦»æŸå¤±ï¼Œç›´æ¥çº¦æŸåƒç´ è·ç¦»
            
            # 1. å½’ä¸€åŒ–åæ ‡çš„é‡å»ºæŸå¤±ï¼ˆä¿æŒï¼‰
            position_weights = torch.ones(config.seq_len, device=predicted_scanpaths.device)
            position_weights[0] = 3.0  # ç¬¬ä¸€æ­¥æƒé‡æœ€é«˜
            position_weights[1:5] = 2.0  # å‰5æ­¥æƒé‡è¾ƒé«˜
            position_weights[5:10] = 1.5  # 5-10æ­¥æƒé‡é€‚ä¸­
            position_weights = position_weights.unsqueeze(0).unsqueeze(-1)  # (1, seq_len, 1)
            
            # åŠ æƒMSEæŸå¤±ï¼ˆå½’ä¸€åŒ–åæ ‡ï¼‰
            squared_errors = (predicted_scanpaths - true_scanpaths) ** 2  # (B, T, 2)
            weighted_errors = squared_errors * position_weights  # (B, T, 2)
            reconstruction_loss_norm = weighted_errors.mean()
            
            # 2. åƒç´ çº§è·ç¦»æŸå¤±ï¼ˆå…³é”®ä¿®å¤ï¼šç›´æ¥çº¦æŸåƒç´ è·ç¦»ï¼‰
            # è½¬æ¢ä¸ºåƒç´ åæ ‡
            h, w = config.image_size
            pred_pixels = predicted_scanpaths.clone()
            pred_pixels[:, :, 0] = pred_pixels[:, :, 0] * w  # Xåæ ‡
            pred_pixels[:, :, 1] = pred_pixels[:, :, 1] * h  # Yåæ ‡
            
            true_pixels = true_scanpaths.clone()
            true_pixels[:, :, 0] = true_pixels[:, :, 0] * w  # Xåæ ‡
            true_pixels[:, :, 1] = true_pixels[:, :, 1] * h  # Yåæ ‡
            
            # è®¡ç®—åƒç´ è·ç¦»ï¼ˆå½’ä¸€åŒ–åˆ°å›¾åƒå¯¹è§’çº¿é•¿åº¦ï¼‰
            pixel_distances = torch.norm(pred_pixels - true_pixels, p=2, dim=-1)  # (B, T)
            diagonal_length = np.sqrt(w**2 + h**2)  # å›¾åƒå¯¹è§’çº¿é•¿åº¦ï¼Œç”¨äºå½’ä¸€åŒ–
            pixel_distances_norm = pixel_distances / diagonal_length  # å½’ä¸€åŒ–åˆ°[0, 1]
            
            # RECé£æ ¼çš„æŸå¤±ï¼šæƒ©ç½šè·ç¦»è¶…è¿‡é˜ˆå€¼çš„ç‚¹ï¼ˆå½’ä¸€åŒ–é˜ˆå€¼ï¼‰
            # é˜ˆå€¼12åƒç´ ï¼Œå½’ä¸€åŒ–åˆ°å¯¹è§’çº¿é•¿åº¦
            rec_threshold_norm = 12.0 / diagonal_length
            rec_threshold_pixels = 12.0  # åƒç´ é˜ˆå€¼
            
            # ç¡¬çº¦æŸï¼šå¯¹è·ç¦»>12åƒç´ çš„ç‚¹å¯¹ä½¿ç”¨æ›´å¼ºçš„æƒ©ç½šï¼ˆFocalæŸå¤±é£æ ¼ï¼‰
            # è·ç¦»è¶Šè¿œï¼Œæƒ©ç½šè¶Šå¤§ï¼ˆæŒ‡æ•°å¢é•¿ï¼‰
            pixel_distances_abs = pixel_distances  # (B, T) ç»å¯¹åƒç´ è·ç¦»
            far_mask = pixel_distances_abs > rec_threshold_pixels  # (B, T) è·ç¦»>12åƒç´ çš„mask
            
            # å¯¹äºè¿œè·ç¦»ç‚¹å¯¹ï¼Œä½¿ç”¨æŒ‡æ•°æƒ©ç½šï¼šexp((distance - threshold) / threshold)
            # è¿™æ ·è·ç¦»è¶Šè¿œï¼Œæƒ©ç½šå¢é•¿è¶Šå¿«
            far_distances = pixel_distances_abs[far_mask]  # åªå¯¹è¿œè·ç¦»ç‚¹è®¡ç®—
            if len(far_distances) > 0:
                # å½’ä¸€åŒ–åˆ°é˜ˆå€¼ï¼Œç„¶åæŒ‡æ•°å¢é•¿
                normalized_far = (far_distances - rec_threshold_pixels) / rec_threshold_pixels
                rec_penalty_far = torch.mean(torch.exp(normalized_far * 2.0))  # æŒ‡æ•°æƒ©ç½š
            else:
                rec_penalty_far = torch.tensor(0.0, device=predicted_scanpaths.device)
            
            # å¯¹äºæ‰€æœ‰ç‚¹ï¼Œä½¿ç”¨å¹³æ–¹æƒ©ç½šï¼ˆå½’ä¸€åŒ–ï¼‰
            rec_penalty_all = torch.mean((pixel_distances_norm - rec_threshold_norm).clamp(min=0.0) ** 2)
            
            # ç»„åˆRECæƒ©ç½šï¼šæ‰€æœ‰ç‚¹çš„æƒ©ç½š + è¿œè·ç¦»ç‚¹çš„é¢å¤–æƒ©ç½š
            rec_penalty = rec_penalty_all + 3.0 * rec_penalty_far  # è¿œè·ç¦»æƒ©ç½šæƒé‡3.0
            
            # åƒç´ çº§MSEæŸå¤±ï¼ˆå½’ä¸€åŒ–åˆ°å›¾åƒå°ºå¯¸ï¼‰
            pixel_diff_norm = (pred_pixels - true_pixels) / diagonal_length  # å½’ä¸€åŒ–å·®å€¼
            pixel_mse = torch.mean(pixel_diff_norm ** 2)
            
            # åƒç´ çº§L1æŸå¤±ï¼ˆå½’ä¸€åŒ–ï¼‰
            pixel_l1 = torch.mean(torch.abs(pixel_diff_norm))
            
            # ç»„åˆé‡å»ºæŸå¤±ï¼šå½’ä¸€åŒ–åæ ‡æŸå¤± + å½’ä¸€åŒ–åƒç´ çº§æŸå¤±
            # å…³é”®ä¿®å¤ï¼šå¤§å¹…æé«˜RECæƒ©ç½šæƒé‡ï¼Œä»2.0æé«˜åˆ°8.0ï¼Œç¡®ä¿ç‚¹å¯¹ç‚¹åŒ¹é…
            reconstruction_loss = reconstruction_loss_norm + 0.5 * pixel_mse + 0.5 * pixel_l1 + 8.0 * rec_penalty

            # 2. KLæ•£åº¦æ­£åˆ™åŒ–ï¼ˆå¢å¼ºæ­£åˆ™åŒ–ï¼Œè§£å†³è¿‡æ‹Ÿåˆï¼‰
            # ä¿®å¤ï¼šæé«˜KLæƒé‡ï¼Œä»0.0003-0.0007æé«˜åˆ°0.002-0.01
            kl_loss = -0.5 * torch.sum(1 + logvars - mus.pow(2) - logvars.exp())
            kl_loss = kl_loss / (mus.size(0) * mus.size(1))  # å½’ä¸€åŒ–

            # 3. ç©ºé—´è¦†ç›–æŸå¤±ï¼ˆä¿æŒé€‚åº¦çº¦æŸï¼‰
            spatial_coverage_loss = compute_spatial_coverage_loss(predicted_scanpaths)

            # 4. è½¨è¿¹å¹³æ»‘æŸå¤±ï¼ˆä¿æŒè½¨è¿¹å¹³æ»‘ï¼‰
            trajectory_smoothness_loss = compute_trajectory_smoothness_loss(predicted_scanpaths, true_scanpaths)

            # 5. æ–¹å‘ä¸€è‡´æ€§æŸå¤±ï¼ˆä¿æŒæ–¹å‘ä¸€è‡´ï¼‰
            direction_consistency_loss = compute_direction_consistency_loss(predicted_scanpaths, true_scanpaths)

            # 6. åºåˆ—å¯¹é½æŸå¤±ï¼ˆçº¦æŸæ‰€æœ‰30æ­¥ï¼Œå¤§å¹…æé«˜æƒé‡ï¼‰
            sequence_alignment_loss = compute_sequence_alignment_loss(predicted_scanpaths, true_scanpaths)

            # 7. è¿åŠ¨ä¸€è‡´æ€§æŸå¤±ï¼ˆæ–¹æ¡ˆCï¼šæ˜¾å¼çº¦æŸæ–¹å‘å’Œæ­¥é•¿ï¼‰
            motion_consistency_loss = compute_motion_consistency_loss(predicted_scanpaths, true_scanpaths)

            # 8. è¾¹ç•Œçº¦æŸ
            boundary_min = 0.02
            boundary_max = 0.98
            below_boundary = (predicted_scanpaths < boundary_min).float()
            above_boundary = (predicted_scanpaths > boundary_max).float()
            boundary_penalty = torch.mean(
                below_boundary * (boundary_min - predicted_scanpaths) ** 2 +
                above_boundary * (predicted_scanpaths - boundary_max) ** 2
            )

            # ========== ä¼˜åŒ–ç‰ˆæŸå¤±æƒé‡ï¼šä¿®å¤RECä¸º0å’Œè¿‡æ‹Ÿåˆé—®é¢˜ ==========
            # å…³é”®ä¼˜åŒ–ï¼š
            # 1. å¤§å¹…æé«˜åºåˆ—å¯¹é½æŸå¤±æƒé‡ï¼ˆä»3.0-4.0æé«˜åˆ°12.0-20.0ï¼‰ï¼Œç¡®ä¿ç‚¹å¯¹ç‚¹åŒ¹é…
            # 2. å¢å¼ºKLæ­£åˆ™åŒ–ï¼ˆä»0.0003-0.0007æé«˜åˆ°0.002-0.01ï¼‰ï¼Œè§£å†³è¿‡æ‹Ÿåˆ
            # 3. ä¿æŒreconstructionæƒé‡é€‚ä¸­ï¼Œå› ä¸ºå·²ç»æ·»åŠ äº†é«˜æƒé‡çš„åƒç´ çº§æŸå¤±
            # 4. æ¸è¿›å¼æƒé‡è°ƒæ•´ï¼Œç¡®ä¿è®­ç»ƒç¨³å®š
            if epoch <= 10:
                # æ—©æœŸï¼šé‡ç‚¹å­¦ä¹ ç‚¹å¯¹ç‚¹åŒ¹é…ï¼Œå¼ºçº¦æŸåºåˆ—å¯¹é½
                weights = {
                    'reconstruction': 5.0,  # é€‚ä¸­æƒé‡ï¼ˆå› ä¸ºå·²æœ‰é«˜æƒé‡åƒç´ çº§æŸå¤±ï¼‰
                    'kl': 0.002,  # æé«˜KLæƒé‡ï¼Œå¢å¼ºæ­£åˆ™åŒ–ï¼ˆä»0.0003æé«˜åˆ°0.002ï¼‰
                    'spatial_coverage': 0.5,  # é™ä½ï¼Œé¿å…è¿‡åº¦çº¦æŸ
                    'trajectory_smoothness': 0.1,  # é™ä½ï¼Œå…è®¸æ›´çµæ´»çš„è·¯å¾„
                    'direction_consistency': 0.1,  # é™ä½ï¼Œé¿å…è¿‡åº¦çº¦æŸ
                    'sequence_alignment': 12.0,  # å¤§å¹…æé«˜ï¼Œç¡®ä¿ç‚¹å¯¹ç‚¹åŒ¹é…ï¼ˆä»3.0æé«˜åˆ°12.0ï¼‰
                    'motion_consistency': 0.15,  # é€‚åº¦è¿åŠ¨è¿ç»­æ€§
                    'boundary': 0.1
                }
            elif epoch <= 25:
                # ä¸­æœŸï¼šå¹³è¡¡å„é¡¹æŸå¤±ï¼Œé€æ¸å¢åŠ æ­£åˆ™åŒ–
                progress = (epoch - 10) / 15.0
                weights = {
                    'reconstruction': 5.0 - 0.5 * progress,  # é€æ¸é™ä½åˆ°4.5
                    'kl': 0.002 + 0.004 * progress,  # é€æ¸å¢åŠ åˆ°0.006ï¼ˆä»0.0007æé«˜åˆ°0.006ï¼‰
                    'spatial_coverage': 0.5 + 0.5 * progress,  # é€æ¸å¢åŠ åˆ°1.0
                    'trajectory_smoothness': 0.1 + 0.2 * progress,  # é€æ¸å¢åŠ åˆ°0.3
                    'direction_consistency': 0.1 + 0.2 * progress,  # é€æ¸å¢åŠ åˆ°0.3
                    'sequence_alignment': 12.0 + 4.0 * progress,  # é€æ¸å¢åŠ åˆ°16.0ï¼ˆä»4.0æé«˜åˆ°16.0ï¼‰
                    'motion_consistency': 0.15 + 0.35 * progress,  # é€æ¸å¢åŠ åˆ°0.5
                    'boundary': 0.1
                }
            else:
                # åæœŸï¼šç²¾ç»†è°ƒä¼˜ï¼Œä¿æŒå¼ºçº¦æŸ
                weights = {
                    'reconstruction': 4.5,  # æœ€ç»ˆæƒé‡ï¼ˆé€‚ä¸­ï¼Œå› ä¸ºå·²æœ‰é«˜æƒé‡åƒç´ çº§æŸå¤±ï¼‰
                    'kl': 0.01,  # æœ€ç»ˆæƒé‡ï¼ˆå¤§å¹…æé«˜ï¼Œä»0.0007æé«˜åˆ°0.01ï¼Œè§£å†³è¿‡æ‹Ÿåˆï¼‰
                    'spatial_coverage': 1.0,  # æœ€ç»ˆæƒé‡
                    'trajectory_smoothness': 0.3,  # æœ€ç»ˆæƒé‡
                    'direction_consistency': 0.3,  # æœ€ç»ˆæƒé‡
                    'sequence_alignment': 20.0,  # æœ€ç»ˆæƒé‡ï¼ˆå¤§å¹…æé«˜ï¼Œä»4.0æé«˜åˆ°20.0ï¼Œç¡®ä¿ç‚¹å¯¹ç‚¹åŒ¹é…ï¼‰
                    'motion_consistency': 0.5,  # æœ€ç»ˆæƒé‡
                    'boundary': 0.1
                }

            # è®¡ç®—æ€»æŸå¤±ï¼ˆæ·»åŠ motion_consistencyé¡¹ï¼‰
            loss = (
                    weights['reconstruction'] * reconstruction_loss +
                    weights['kl'] * kl_loss +
                    weights['spatial_coverage'] * spatial_coverage_loss +
                    weights['trajectory_smoothness'] * trajectory_smoothness_loss +
                    weights['direction_consistency'] * direction_consistency_loss +
                    weights['sequence_alignment'] * sequence_alignment_loss +
                    weights['motion_consistency'] * motion_consistency_loss +
                    weights['boundary'] * boundary_penalty
            )
            
            # æ¢¯åº¦è£å‰ªï¼Œé¿å…è®­ç»ƒä¸ç¨³å®š
            # æ³¨æ„ï¼šè¿™é‡Œå…ˆè®¡ç®—lossï¼Œåå‘ä¼ æ’­æ—¶å†è£å‰ª

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            # æ¢¯åº¦è£å‰ªï¼šé™ä½max_normï¼Œé¿å…è®­ç»ƒä¸ç¨³å®š
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)  # ä»1.0é™åˆ°0.5
            optimizer.step()

            # è®¡ç®—ä½ç½®è¯¯å·®
            position_weights_error = torch.ones(config.seq_len, device=predicted_scanpaths.device)
            if epoch <= 80:
                position_weights_error[0] = 2.5
                position_weights_error[1:5] = 1.8
                position_weights_error[5:10] = 1.3
            else:
                position_weights_error[0] = 2.0
                position_weights_error[1:5] = 1.5
                position_weights_error[5:10] = 1.2

            # åŠ æƒä½ç½®è¯¯å·®
            weighted_errors = torch.norm(
                predicted_scanpaths - true_scanpaths,
                dim=-1
            ) * position_weights_error.unsqueeze(0)
            position_error = weighted_errors.mean() / position_weights_error.mean()  # å½’ä¸€åŒ–ä»¥ä¿æŒåŸæœ‰å°ºåº¦

            # ç´¯ç§¯æŒ‡æ ‡
            epoch_loss += loss.item()
            epoch_position_error += position_error.item()

            # æ›´æ–°è¿›åº¦æ¡
            if (batch_idx + 1) % config.log_interval == 0:
                avg_loss = epoch_loss / (batch_idx + 1)
                avg_error = epoch_position_error / (batch_idx + 1)
                train_bar.set_postfix({
                    'Loss': f"{avg_loss:.4f}",
                    'PosErr': f"{avg_error:.4f}",
                    'TF': f"{teacher_forcing_ratio:.3f}",
                    'SeqAlign': f"{sequence_alignment_loss.item():.4f}",
                    'Recon': f"{reconstruction_loss.item():.4f}",
                    'KL': f"{kl_loss.item():.5f}",
                })

        # å¹³å‡è®­ç»ƒæŒ‡æ ‡
        num_batches = len(train_loader)
        epoch_loss /= num_batches
        epoch_position_error /= num_batches

        # æ‰“å°è®­ç»ƒç»“æœ
        print(f"\nè®­ç»ƒç»“æœ:")
        print(f"  Loss: {epoch_loss:.4f}")
        print(f"  PositionError: {epoch_position_error:.4f}")

        # éªŒè¯
        if epoch % config.val_interval == 0:
            print(f"\néªŒè¯...")
            model.eval()
            val_loss = 0
            val_position_error = 0

            val_bar = tqdm(test_loader, desc="éªŒè¯")
            with torch.no_grad():
                for batch in val_bar:
                    images = batch['image'].to(config.device)
                    true_scanpaths = batch['scanpath'].to(config.device)

                    # å‰å‘ä¼ æ’­ - éªŒè¯æ¨¡å¼
                    # ä¿®å¤ï¼šéªŒè¯æ—¶ä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„Teacher Forcingæ¯”ä¾‹ï¼Œç»Ÿä¸€è®­ç»ƒå’ŒéªŒè¯ç­–ç•¥
                    val_teacher_forcing = teacher_forcing_ratio  # ä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„TFæ¯”ä¾‹
                    result = model(images, gt_scanpaths=true_scanpaths,
                                   teacher_forcing_ratio=val_teacher_forcing,
                                   enable_early_stop=False,
                                   use_gt_start=True)  # éªŒè¯æ—¶ä¹Ÿä½¿ç”¨çœŸå®èµ·å§‹ç‚¹
                    # å®‰å…¨è§£åŒ…ï¼šæ— è®ºè¿”å›3ä¸ªè¿˜æ˜¯5ä¸ªå€¼ï¼Œéƒ½åªå–å‰3ä¸ª
                    predicted_scanpaths = result[0]
                    mus = result[1]
                    logvars = result[2]

                    # ========== ä¿®å¤ç‰ˆéªŒè¯æŸå¤±ï¼ˆä¸è®­ç»ƒä¸€è‡´ï¼‰==========
                    # 1. é‡æ„æŸå¤±ï¼ˆä½¿ç”¨åŠ æƒMSE + åƒç´ çº§æŸå¤±ï¼Œä¸è®­ç»ƒä¸€è‡´ï¼‰
                    position_weights = torch.ones(config.seq_len, device=predicted_scanpaths.device)
                    position_weights[0] = 3.0
                    position_weights[1:5] = 2.0
                    position_weights[5:10] = 1.5
                    position_weights = position_weights.unsqueeze(0).unsqueeze(-1)
                    
                    squared_errors = (predicted_scanpaths - true_scanpaths) ** 2
                    weighted_errors = squared_errors * position_weights
                    reconstruction_loss_norm = weighted_errors.mean()
                    
                    # åƒç´ çº§è·ç¦»æŸå¤±
                    h, w = config.image_size
                    pred_pixels = predicted_scanpaths.clone()
                    pred_pixels[:, :, 0] = pred_pixels[:, :, 0] * w
                    pred_pixels[:, :, 1] = pred_pixels[:, :, 1] * h
                    
                    true_pixels = true_scanpaths.clone()
                    true_pixels[:, :, 0] = true_pixels[:, :, 0] * w
                    true_pixels[:, :, 1] = true_pixels[:, :, 1] * h
                    
                    pixel_distances = torch.norm(pred_pixels - true_pixels, p=2, dim=-1)
                    diagonal_length = np.sqrt(w**2 + h**2)
                    pixel_distances_norm = pixel_distances / diagonal_length
                    
                    # éªŒè¯æŸå¤±è®¡ç®—ï¼ˆä¸è®­ç»ƒä¸€è‡´ï¼‰
                    rec_threshold_norm = 12.0 / diagonal_length
                    rec_threshold_pixels = 12.0
                    
                    # ç¡¬çº¦æŸï¼šå¯¹è·ç¦»>12åƒç´ çš„ç‚¹å¯¹ä½¿ç”¨æ›´å¼ºçš„æƒ©ç½š
                    pixel_distances_abs = pixel_distances
                    far_mask = pixel_distances_abs > rec_threshold_pixels
                    
                    if len(pixel_distances_abs[far_mask]) > 0:
                        far_distances = pixel_distances_abs[far_mask]
                        normalized_far = (far_distances - rec_threshold_pixels) / rec_threshold_pixels
                        rec_penalty_far = torch.mean(torch.exp(normalized_far * 2.0))
                    else:
                        rec_penalty_far = torch.tensor(0.0, device=predicted_scanpaths.device)
                    
                    rec_penalty_all = torch.mean((pixel_distances_norm - rec_threshold_norm).clamp(min=0.0) ** 2)
                    rec_penalty = rec_penalty_all + 3.0 * rec_penalty_far
                    
                    pixel_diff_norm = (pred_pixels - true_pixels) / diagonal_length
                    pixel_mse = torch.mean(pixel_diff_norm ** 2)
                    pixel_l1 = torch.mean(torch.abs(pixel_diff_norm))
                    
                    reconstruction_loss = reconstruction_loss_norm + 0.5 * pixel_mse + 0.5 * pixel_l1 + 8.0 * rec_penalty

                    # 2. KLæ•£åº¦æ­£åˆ™åŒ–
                    kl_loss = -0.5 * torch.sum(1 + logvars - mus.pow(2) - logvars.exp())
                    kl_loss = kl_loss / (mus.size(0) * mus.size(1))

                    # 3. ç©ºé—´è¦†ç›–æŸå¤±
                    spatial_coverage_loss = compute_spatial_coverage_loss(predicted_scanpaths)

                    # 4. è½¨è¿¹å¹³æ»‘æŸå¤±
                    trajectory_smoothness_loss = compute_trajectory_smoothness_loss(predicted_scanpaths, true_scanpaths)

                    # 5. æ–¹å‘ä¸€è‡´æ€§æŸå¤±
                    direction_consistency_loss = compute_direction_consistency_loss(predicted_scanpaths, true_scanpaths)

                    # 6. åºåˆ—å¯¹é½æŸå¤±
                    sequence_alignment_loss = compute_sequence_alignment_loss(predicted_scanpaths, true_scanpaths)

                    # 7. è¿åŠ¨ä¸€è‡´æ€§æŸå¤±
                    motion_consistency_loss = compute_motion_consistency_loss(predicted_scanpaths, true_scanpaths)

                    # 8. è¾¹ç•Œçº¦æŸ
                    boundary_min = 0.02
                    boundary_max = 0.98
                    below_boundary = (predicted_scanpaths < boundary_min).float()
                    above_boundary = (predicted_scanpaths > boundary_max).float()
                    boundary_penalty = torch.mean(
                        below_boundary * (boundary_min - predicted_scanpaths) ** 2 +
                        above_boundary * (predicted_scanpaths - boundary_max) ** 2
                    )

                    # ä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„æƒé‡ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
                    if epoch <= 10:
                        weights = {
                            'reconstruction': 5.0,
                            'kl': 0.002,
                            'spatial_coverage': 0.5,
                            'trajectory_smoothness': 0.1,
                            'direction_consistency': 0.1,
                            'sequence_alignment': 12.0,
                            'motion_consistency': 0.15,
                            'boundary': 0.1
                        }
                    elif epoch <= 25:
                        progress = (epoch - 10) / 15.0
                        weights = {
                            'reconstruction': 5.0 - 0.5 * progress,
                            'kl': 0.002 + 0.004 * progress,
                            'spatial_coverage': 0.5 + 0.5 * progress,
                            'trajectory_smoothness': 0.1 + 0.2 * progress,
                            'direction_consistency': 0.1 + 0.2 * progress,
                            'sequence_alignment': 12.0 + 4.0 * progress,
                            'motion_consistency': 0.15 + 0.35 * progress,
                            'boundary': 0.1
                        }
                    else:
                        weights = {
                            'reconstruction': 4.5,
                            'kl': 0.01,
                            'spatial_coverage': 1.0,
                            'trajectory_smoothness': 0.3,
                            'direction_consistency': 0.3,
                            'sequence_alignment': 20.0,
                            'motion_consistency': 0.5,
                            'boundary': 0.1
                        }

                    # è®¡ç®—æ€»æŸå¤±ï¼ˆåŒ…å«motion_consistencyé¡¹ï¼‰
                    loss = (
                            weights['reconstruction'] * reconstruction_loss +
                            weights['kl'] * kl_loss +
                            weights['spatial_coverage'] * spatial_coverage_loss +
                            weights['trajectory_smoothness'] * trajectory_smoothness_loss +
                            weights['direction_consistency'] * direction_consistency_loss +
                            weights['sequence_alignment'] * sequence_alignment_loss +
                            weights['motion_consistency'] * motion_consistency_loss +
                            weights['boundary'] * boundary_penalty
                    )

                    # è®¡ç®—ä½ç½®è¯¯å·®
                    position_error = torch.norm(
                        predicted_scanpaths - true_scanpaths,
                        dim=-1
                    ).mean()

                    val_loss += loss.item()
                    val_position_error += position_error.item()

            # å¹³å‡éªŒè¯æŒ‡æ ‡
            num_val_batches = len(test_loader)
            val_loss /= num_val_batches
            val_position_error /= num_val_batches

            print(f"\néªŒè¯ç»“æœ:")
            print(f"  Loss: {val_loss:.4f}")
            print(f"  PositionError: {val_position_error:.4f}")

            # å­¦ä¹ ç‡è°ƒåº¦ - ExponentialLRåœ¨æ¯ä¸ªepochåè‡ªåŠ¨è¡°å‡
            current_lr = optimizer.param_groups[0]['lr']
            print(f"  Learning Rate: {current_lr:.6f}")

            # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼šä¼˜å…ˆåŸºäºä½ç½®è¯¯å·®ï¼Œä¹Ÿè€ƒè™‘æŸå¤±
            # ä¿®å¤ï¼šæ­£ç¡®è®¡ç®—patienceï¼Œè€ƒè™‘éªŒè¯é—´éš”
            save_model = False
            improved = False
            
            if val_position_error < best_val_position_error:
                best_val_position_error = val_position_error
                save_model = True
                improved = True
                patience_counter = 0  # é‡ç½®æ—©åœè®¡æ•°å™¨ï¼ˆåŸºäºä½ç½®è¯¯å·®ï¼‰
                print(f"  âœ… éªŒè¯ä½ç½®è¯¯å·®æ”¹å–„: {val_position_error:.4f} (æ–°æœ€ä½³)")

            if val_loss < best_val_loss:
                best_val_loss = val_loss
                if not save_model:  # å¦‚æœä½ç½®è¯¯å·®æ²¡æ”¹å–„ä½†æŸå¤±æ”¹å–„äº†ï¼Œä¹Ÿä¿å­˜
                    save_model = True

            if save_model:
                best_path = os.path.join(config.checkpoint_dir, 'best_model.pth')
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'best_loss': best_val_loss,
                    'best_position_error': best_val_position_error,
                }, best_path)
                print(f"  ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")
                print(f"     æœ€ä½³éªŒè¯ä½ç½®è¯¯å·®: {best_val_position_error:.4f}")
                print(f"     æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
            
            # ä¿®å¤ï¼šåªæœ‰åœ¨éªŒè¯æ—¶ä¸”æœªæ”¹å–„æ—¶æ‰å¢åŠ patience
            if not improved:
                # è®¡ç®—ä»ä¸Šæ¬¡éªŒè¯åˆ°ç°åœ¨çš„epochæ•°
                epochs_since_last_val = epoch - last_val_epoch
                patience_counter += epochs_since_last_val
                print(f"  âš ï¸ éªŒè¯ä½ç½®è¯¯å·®æœªæ”¹å–„ (patience: {patience_counter}/{early_stopping_patience})")
                print(f"     å½“å‰: {val_position_error:.4f}, æœ€ä½³: {best_val_position_error:.4f}")
            
            last_val_epoch = epoch

            # æ—©åœæ£€æŸ¥ï¼šåŸºäºä½ç½®è¯¯å·®
            if patience_counter >= early_stopping_patience:
                print(f"\nâ¹ï¸ æ—©åœè§¦å‘ï¼éªŒè¯ä½ç½®è¯¯å·®å·²ç»{patience_counter}ä¸ªepochæ²¡æœ‰æ”¹å–„")
                print(f"æœ€ä½³éªŒè¯ä½ç½®è¯¯å·®: {best_val_position_error:.4f}")
                print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
                break

        # ä¿å­˜æ£€æŸ¥ç‚¹
        if epoch % config.save_interval == 0:
            checkpoint_path = os.path.join(
                config.checkpoint_dir,
                f'checkpoint_epoch_{epoch}.pth'
            )
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
            }, checkpoint_path)
            print(f"  ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")

        # è·å–å½“å‰å­¦ä¹ ç‡
        current_lr = optimizer.param_groups[0]['lr']

        # è®°å½•æ—¥å¿—
        epoch_log = {
            'epoch': epoch,
            'learning_rate': current_lr,
            'train': {
                'loss': epoch_loss,
                'position_error': epoch_position_error,
            },
        }
        if epoch % config.val_interval == 0:
            epoch_log['val'] = {
                'loss': val_loss,
                'position_error': val_position_error,
            }

        training_log['epochs'].append(epoch_log)

        # ä¿å­˜è®­ç»ƒæ—¥å¿—
        log_path = os.path.join(config.log_dir, 'training_log.json')
        with open(log_path, 'w') as f:
            json.dump(training_log, f, indent=2)

        # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆæ¯ä¸ªepochç»“æŸåï¼‰
        scheduler.step()

    print("\nè®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
    print(f"æœ€ä½³éªŒè¯ä½ç½®è¯¯å·®: {best_val_position_error:.4f}")
    print(f"\nä¼˜åŒ–è¯´æ˜ï¼š")
    print(f"  1. å¤§å¹…æé«˜åºåˆ—å¯¹é½æŸå¤±æƒé‡ï¼ˆ12.0-20.0ï¼‰ï¼Œä¿®å¤RECä¸º0é—®é¢˜")
    print(f"  2. å¢å¼ºKLæ­£åˆ™åŒ–ï¼ˆ0.002-0.01ï¼‰ï¼Œè§£å†³è¿‡æ‹Ÿåˆé—®é¢˜")
    print(f"  3. æé«˜RECæƒ©ç½šæƒé‡ï¼ˆ8.0ï¼‰ï¼Œç¡®ä¿ç‚¹å¯¹ç‚¹åŒ¹é…")
    print(f"  4. é™ä½åˆå§‹å­¦ä¹ ç‡ï¼ˆ0.000048ï¼‰ï¼Œå»¶é•¿warmupï¼ˆ10 epochsï¼‰")
    print(f"  5. ç»Ÿä¸€è®­ç»ƒå’ŒéªŒè¯çš„Teacher Forcingç­–ç•¥")
    print(f"\nä¸‹ä¸€æ­¥ï¼šä½¿ç”¨ evaluate_fixed.py è¯„ä¼°æ¨¡å‹ï¼Œæ£€æŸ¥RECæŒ‡æ ‡æ˜¯å¦æ”¹å–„")


if __name__ == '__main__':
    train()
