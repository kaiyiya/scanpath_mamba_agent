"""
Mamba-Adaptiveæ‰«æè·¯å¾„æ¨¡å‹è®­ç»ƒè„šæœ¬
ç»“åˆ Mamba + AdaptiveNN Focusæœºåˆ¶
"""
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import json
from datetime import datetime
from tqdm import tqdm
from pathlib import Path

from config_mamba_adaptive import MambaAdaptiveConfig
from data.dataset import create_dataloaders
from models.mamba_adaptive_scanpath import MambaAdaptiveScanpath


def train():
    """è®­ç»ƒä¸»å‡½æ•°"""
    config = MambaAdaptiveConfig()

    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(config.log_dir, exist_ok=True)
    os.makedirs(config.checkpoint_dir, exist_ok=True)

    # åŠ è½½æ•°æ®
    print("åŠ è½½æ•°æ®...")
    train_loader, test_loader = create_dataloaders(config)
    print(f"è®­ç»ƒé›†: {len(train_loader)} batches")
    print(f"æµ‹è¯•é›†: {len(test_loader)} batches")

    # åˆ›å»ºæ¨¡å‹
    print("\nåˆ›å»ºMamba-Adaptiveæ¨¡å‹ï¼ˆç»“åˆFocusæœºåˆ¶ï¼‰...")
    model = MambaAdaptiveScanpath(config).to(config.device)
    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M")

    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.learning_rate,
        weight_decay=config.weight_decay
    )

    # å­¦ä¹ ç‡è°ƒåº¦å™¨ - ä½¿ç”¨ä½™å¼¦é€€ç«ï¼ˆæ›´å¥½çš„æ”¶æ•›æ€§ï¼‰
    # è®­ç»ƒç›®æ ‡ï¼šå­¦ä¹ ç‡åœ¨å‰åŠç¨‹è¾ƒé«˜ï¼ˆå¿«é€Ÿå­¦ä¹ ï¼‰ï¼ŒååŠç¨‹è¾ƒä½ï¼ˆç²¾ç»†è°ƒä¼˜ï¼‰
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=config.num_epochs,
        eta_min=config.learning_rate * 0.01  # æœ€å°å­¦ä¹ ç‡ä¸ºåˆå§‹çš„1%
    )

    # æ—©åœæœºåˆ¶ï¼šåŸºäºéªŒè¯ä½ç½®è¯¯å·®è€Œä¸æ˜¯æŸå¤±
    # æ”¹è¿›ï¼šä½¿ç”¨ä½ç½®è¯¯å·®ä½œä¸ºæ—©åœæŒ‡æ ‡ï¼Œæ›´ç¬¦åˆä¸»è¦ç›®æ ‡
    best_val_position_error = float('inf')
    patience_counter = 0
    early_stopping_patience = 20  # å¢åŠ åˆ°20ï¼Œç»™æ¨¡å‹æ›´å¤šæœºä¼š
    best_val_loss = float('inf')  # ä»ç„¶è®°å½•ï¼Œä½†ç”¨äºä¿å­˜æ¨¡å‹

    # è®­ç»ƒæ—¥å¿—
    training_log = {
        'config': {
            'seq_len': config.seq_len,
            'batch_size': config.batch_size,
            'learning_rate': config.learning_rate,
            'num_epochs': config.num_epochs,
            'feature_dim': config.feature_dim,
            'd_state': config.d_state,
            'focus_patch_size': config.focus_patch_size,
        },
        'epochs': []
    }

    # è®­ç»ƒå¾ªç¯
    print("\nå¼€å§‹è®­ç»ƒ...")
    best_loss = float('inf')

    for epoch in range(1, config.num_epochs + 1):
        print(f"\n{'='*80}")
        print(f"Epoch {epoch}/{config.num_epochs}")
        print(f"{'='*80}")

        # è®­ç»ƒ
        model.train()
        epoch_loss = 0
        epoch_position_error = 0

        train_bar = tqdm(train_loader, desc="è®­ç»ƒ")
        for batch_idx, batch in enumerate(train_bar):
            images = batch['image'].to(config.device)
            true_scanpaths = batch['scanpath'].to(config.device)

            # å‰å‘ä¼ æ’­ - ä¼ é€’çœŸå®ä½ç½®ç”¨äºTeacher Forcing
            # æ”¹è¿›Teacher Forcingç­–ç•¥ï¼šæ›´æ…¢çš„è¡°å‡ï¼Œè®©æ¨¡å‹å……åˆ†å­¦ä¹ 
            # æ–¹æ¡ˆ5ï¼šè°ƒæ•´Teacher Forcingç­–ç•¥
            if epoch <= 100:
                # é˜¶æ®µ1ï¼šä»0.7ç¼“æ…¢é™åˆ°0.4
                teacher_forcing_ratio = 0.7 - 0.3 * (epoch / 100.0)
            elif epoch <= 200:
                # é˜¶æ®µ2ï¼šä»0.4ç¼“æ…¢é™åˆ°0.2
                teacher_forcing_ratio = 0.4 - 0.2 * ((epoch - 100) / 100.0)
            else:
                # é˜¶æ®µ3ï¼šä¿æŒ0.2
                teacher_forcing_ratio = 0.2

            # è®­ç»ƒæ—¶æ˜¾å¼è®¾ç½®enable_early_stop=Falseï¼Œç¡®ä¿è¿”å›3ä¸ªå€¼
            predicted_scanpaths, mus, logvars = model(
                images,
                gt_scanpaths=true_scanpaths,
                teacher_forcing_ratio=teacher_forcing_ratio,
                enable_early_stop=False
            )

            # è®¡ç®—æŸå¤±å‡½æ•° - VAEæ¡†æ¶ï¼šé‡æ„æŸå¤± + KLæ•£åº¦æ­£åˆ™åŒ–
            # 1. é‡æ„æŸå¤±ï¼ˆå‡†ç¡®åŒ¹é…çœŸå®è·¯å¾„ï¼‰
            # ä½¿ç”¨æ ‡å‡†MSEï¼ˆç”¨äºç›‘æ§ï¼‰ï¼Œå®é™…æŸå¤±ä½¿ç”¨åŠ æƒMSEï¼ˆè§ä¸‹æ–¹ï¼‰
            reconstruction_loss = nn.functional.mse_loss(predicted_scanpaths, true_scanpaths)

            # 2. KLæ•£åº¦æ­£åˆ™åŒ–ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
            # KL(q(z|x) || p(z)) = -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
            kl_loss = -0.5 * torch.sum(1 + logvars - mus.pow(2) - logvars.exp())
            kl_loss = kl_loss / (mus.size(0) * mus.size(1))  # å½’ä¸€åŒ–

            # 3. Beta-VAEï¼šæ§åˆ¶KLæ•£åº¦çš„æƒé‡
            # æ–¹æ¡ˆ6ï¼šé™ä½VAEçš„KLæ•£åº¦æƒé‡ï¼Œå‡å°‘éšæœºæ€§
            # è®­ç»ƒç›®æ ‡ï¼šä¿æŒVAEçš„éšæœºæ€§ï¼Œä½†ä¸å½±å“ä¸»è¦çš„ä½ç½®é¢„æµ‹ç²¾åº¦
            # ç­–ç•¥ï¼šä½¿ç”¨æ›´å°çš„betaå€¼ï¼Œè®©é‡æ„æŸå¤±å ä¸»å¯¼
            beta = min(0.005, 0.001 * (1.01 ** (epoch - 1)))  # ä»0.001å¼€å§‹ï¼Œæœ€å¤šåˆ°0.005

            # 4. è¦†ç›–èŒƒå›´æŸå¤±ï¼šé¼“åŠ±é¢„æµ‹è·¯å¾„è¦†ç›–æ•´ä¸ªå›¾åƒï¼Œé˜²æ­¢è¿‡åº¦èšé›†
            # æ¿€è¿›æ”¹è¿›ï¼šåˆ†åˆ«çº¦æŸXå’ŒYæ–¹å‘çš„è¦†ç›–èŒƒå›´
            pred_min = predicted_scanpaths.min(dim=1)[0]  # (B, 2)
            pred_max = predicted_scanpaths.max(dim=1)[0]  # (B, 2)
            pred_range = pred_max - pred_min  # (B, 2)

            # åˆ†åˆ«å¤„ç†Xå’ŒYæ–¹å‘çš„è¦†ç›–èŒƒå›´
            pred_range_x = pred_range[:, 0]  # (B,)
            pred_range_y = pred_range[:, 1]  # (B,)

            # Yæ–¹å‘éœ€è¦æ›´å¤§çš„è¦†ç›–èŒƒå›´ï¼ˆå½“å‰yå‡å€¼éƒ½åœ¨0.49-0.51ï¼Œå¤ªé›†ä¸­ï¼‰
            min_range_x = 0.3  # xæ–¹å‘æœ€å°è¦†ç›–èŒƒå›´
            min_range_y = 0.25  # yæ–¹å‘æœ€å°è¦†ç›–èŒƒå›´ï¼ˆæé«˜è¦æ±‚ï¼‰

            coverage_loss_x = torch.mean(((min_range_x - pred_range_x).clamp(min=0.0)) ** 2)
            coverage_loss_y = torch.mean(((min_range_y - pred_range_y).clamp(min=0.0)) ** 2)

            # Yæ–¹å‘ç»™äºˆæ›´é«˜æƒé‡
            coverage_loss = coverage_loss_x + 3.0 * coverage_loss_y
            
            # 5. ä½ç½®å¤šæ ·æ€§æŸå¤±ï¼šé¼“åŠ±é¢„æµ‹ä½ç½®å…·æœ‰è¶³å¤Ÿçš„æ–¹å·®
            # æ–¹æ¡ˆ1ï¼šåˆ†åˆ«çº¦æŸXå’ŒYæ–¹å‘çš„å¤šæ ·æ€§ï¼ˆå…³é”®æ”¹è¿›ï¼ï¼‰
            pred_mean = predicted_scanpaths.mean(dim=1)  # (B, 2)
            pred_var = ((predicted_scanpaths - pred_mean.unsqueeze(1)) ** 2).mean(dim=1)  # (B, 2)

            # åˆ†åˆ«å¤„ç†Xå’ŒYæ–¹å‘
            pred_var_x = pred_var[:, 0]  # (B,)
            pred_var_y = pred_var[:, 1]  # (B,)

            # Yæ–¹å‘éœ€è¦æ›´é«˜çš„å¤šæ ·æ€§é˜ˆå€¼ï¼ˆå½“å‰Yæ–¹å‘æ ‡å‡†å·®åªæœ‰0.04-0.07ï¼Œç›®æ ‡0.10-0.15ï¼‰
            # æ¿€è¿›æ”¹è¿›ï¼šå¤§å¹…æé«˜Yæ–¹å‘é˜ˆå€¼å’Œæƒé‡
            min_var_x = 0.015  # xæ–¹å‘æ ‡å‡†å·®çº¦0.12ï¼ˆå·²è¾¾æ ‡ï¼‰
            min_var_y = 0.025  # yæ–¹å‘æ ‡å‡†å·®çº¦0.16ï¼ˆä»0.020æé«˜åˆ°0.025ï¼‰

            # è®¡ç®—å¤šæ ·æ€§æŸå¤±
            diversity_loss_x = torch.mean(((min_var_x - pred_var_x).clamp(min=0.0)) ** 2)
            diversity_loss_y = torch.mean(((min_var_y - pred_var_y).clamp(min=0.0)) ** 2)

            # Yæ–¹å‘ç»™äºˆæ›´é«˜æƒé‡ï¼ˆä»2å€æé«˜åˆ°5å€ï¼ï¼‰
            diversity_loss = diversity_loss_x + 5.0 * diversity_loss_y
            
            # 6. è¾¹ç•Œçº¦æŸï¼šé˜²æ­¢é¢„æµ‹è·‘åˆ°å›¾åƒè¾¹ç•Œå¤–
            # æ–¹æ¡ˆ4ï¼šæ”¾å®½è¾¹ç•Œçº¦æŸï¼Œé™ä½æƒ©ç½šå¼ºåº¦
            # å®šä¹‰åˆç†èŒƒå›´ï¼š[0.02, 0.98]ï¼Œåªé˜²æ­¢å®Œå…¨è¶Šç•Œ
            boundary_min = 0.02  # ä»0.05æ”¾å®½åˆ°0.02
            boundary_max = 0.98  # ä»0.95æ”¾å®½åˆ°0.98

            # æƒ©ç½šè¶…å‡ºè¾¹ç•Œçš„ä½ç½®
            below_boundary = (predicted_scanpaths < boundary_min).float()
            above_boundary = (predicted_scanpaths > boundary_max).float()
            boundary_penalty = torch.mean(
                below_boundary * (boundary_min - predicted_scanpaths) ** 2 +
                above_boundary * (predicted_scanpaths - boundary_max) ** 2
            ) * 2.0  # ä»10.0é™ä½åˆ°2.0ï¼Œåªé˜²æ­¢å®Œå…¨è¶Šç•Œ
            
            # 7. Yæ–¹å‘ä¸­å¿ƒèšé›†æƒ©ç½šï¼šä¸“é—¨é’ˆå¯¹Yæ–¹å‘çš„ä¸­å¿ƒèšé›†é—®é¢˜
            # æ–°å¢ï¼šYæ–¹å‘å‡å€¼è¿‡äºé›†ä¸­åœ¨0.5é™„è¿‘ï¼ˆ0.49-0.51ï¼‰ï¼Œéœ€è¦æƒ©ç½š
            pred_mean_y = pred_mean[:, 1]  # (B,) Yæ–¹å‘çš„å‡å€¼

            # æƒ©ç½šYæ–¹å‘å‡å€¼è¿‡äºæ¥è¿‘0.5
            y_center_dist = torch.abs(pred_mean_y - 0.5)  # è·ç¦»ä¸­å¿ƒçš„è·ç¦»
            # å½“è·ç¦»å°äº0.1æ—¶ï¼ˆå³å‡å€¼åœ¨[0.4, 0.6]ï¼‰ï¼Œç»™äºˆæƒ©ç½š
            y_too_centered = (y_center_dist < 0.1).float()
            y_center_penalty = torch.mean(y_too_centered * (0.1 - y_center_dist) ** 2) * 10.0

            # Xæ–¹å‘ä¸æƒ©ç½šï¼ˆå·²ç»è¶³å¤Ÿåˆ†æ•£ï¼‰
            center_penalty = y_center_penalty
            point_center_penalty = torch.tensor(0.0, device=predicted_scanpaths.device)

            # 8. è½¨è¿¹å¹³æ»‘æ€§æŸå¤±ï¼šé¼“åŠ±ç›¸é‚»ç‚¹ä¹‹é—´çš„è·ç¦»åˆç†ï¼Œä½¿è·¯å¾„è¿è´¯æµç•…
            # æ–¹æ¡ˆ2ï¼šå¢å¼ºè½¨è¿¹å¹³æ»‘æ€§çº¦æŸ
            # è®¡ç®—ç›¸é‚»ç‚¹ä¹‹é—´çš„æ­¥é•¿
            pred_diffs = predicted_scanpaths[:, 1:] - predicted_scanpaths[:, :-1]  # (B, seq_len-1, 2)
            true_diffs = true_scanpaths[:, 1:] - true_scanpaths[:, :-1]  # (B, seq_len-1, 2)
            
            # æ­¥é•¿è·ç¦»
            pred_step_lengths = torch.norm(pred_diffs, p=2, dim=-1)  # (B, seq_len-1)
            true_step_lengths = torch.norm(true_diffs, p=2, dim=-1)  # (B, seq_len-1)
            
            # å¹³æ»‘æ€§æŸå¤±1ï¼šç›¸é‚»ç‚¹è·ç¦»åº”è¯¥ä¸çœŸå®è·¯å¾„ç›¸ä¼¼
            step_length_loss = nn.functional.mse_loss(pred_step_lengths, true_step_lengths)
            
            # å¹³æ»‘æ€§æŸå¤±2ï¼šæƒ©ç½šè¿‡å¤§çš„è·³è·ƒï¼ˆé˜²æ­¢è·¯å¾„è¿‡äºåˆ†æ•£ï¼‰
            # çœŸå®è·¯å¾„çš„æ­¥é•¿é€šå¸¸å°äº0.15ï¼Œæƒ©ç½šè¶…è¿‡0.2çš„è·³è·ƒ
            max_reasonable_step = 0.20
            large_jumps = (pred_step_lengths - max_reasonable_step).clamp(min=0.0)  # åªæƒ©ç½šå¤§äº0.2çš„
            jump_penalty = torch.mean(large_jumps ** 2)
            
            # å¹³æ»‘æ€§æŸå¤±3ï¼šæ–¹å‘ä¸€è‡´æ€§ï¼ˆç›¸é‚»æ–¹å‘å˜åŒ–ä¸åº”è¯¥å¤ªå¤§ï¼‰
            # è®¡ç®—æ–¹å‘å‘é‡
            pred_directions = pred_diffs / (pred_step_lengths.unsqueeze(-1) + 1e-8)  # å½’ä¸€åŒ–æ–¹å‘
            true_directions = true_diffs / (true_step_lengths.unsqueeze(-1) + 1e-8)
            
            # è®¡ç®—ç›¸é‚»æ–¹å‘çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆåº”è¯¥æ˜¯æ­£çš„ï¼Œè¡¨ç¤ºæ–¹å‘å˜åŒ–å¹³ç¼“ï¼‰
            if pred_directions.shape[1] > 1:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç‚¹è®¡ç®—æ–¹å‘å˜åŒ–
                pred_dir_diffs = pred_directions[:, 1:] - pred_directions[:, :-1]  # æ–¹å‘å˜åŒ–
                true_dir_diffs = true_directions[:, 1:] - true_directions[:, :-1]
                direction_loss = nn.functional.mse_loss(
                    torch.norm(pred_dir_diffs, p=2, dim=-1),
                    torch.norm(true_dir_diffs, p=2, dim=-1)
                )
            else:
                direction_loss = torch.tensor(0.0, device=predicted_scanpaths.device)
            
            # å¹³æ»‘æ€§æŸå¤±4ï¼šåŠ é€Ÿåº¦çº¦æŸï¼ˆæ­¥é•¿çš„å˜åŒ–åº”è¯¥å¹³æ»‘ï¼Œä¿è¯è·¯å¾„æµç•…ï¼‰
            # è®¡ç®—åŠ é€Ÿåº¦ï¼šç›¸é‚»æ­¥é•¿ä¹‹é—´çš„å˜åŒ–
            if pred_step_lengths.shape[1] > 1:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ­¥é•¿è®¡ç®—åŠ é€Ÿåº¦
                pred_acceleration = pred_step_lengths[:, 1:] - pred_step_lengths[:, :-1]  # (B, seq_len-2)
                true_acceleration = true_step_lengths[:, 1:] - true_step_lengths[:, :-1]
                acceleration_loss = nn.functional.mse_loss(pred_acceleration, true_acceleration)
            else:
                acceleration_loss = torch.tensor(0.0, device=predicted_scanpaths.device)
            
            # å¹³æ»‘æ€§æŸå¤±5ï¼šæ–¹å‘è¿ç»­æ€§ï¼ˆç›¸é‚»æ–¹å‘åº”è¯¥ç›¸ä¼¼ï¼Œé¿å…çªç„¶è½¬å‘ï¼‰
            if pred_directions.shape[1] > 0:
                # è®¡ç®—ç›¸é‚»æ–¹å‘çš„ä½™å¼¦ç›¸ä¼¼åº¦
                pred_dir_similarity = F.cosine_similarity(
                    pred_directions[:, :-1], 
                    pred_directions[:, 1:], 
                    dim=-1
                )  # (B, seq_len-2)
                true_dir_similarity = F.cosine_similarity(
                    true_directions[:, :-1], 
                    true_directions[:, 1:], 
                    dim=-1
                )
                direction_continuity_loss = nn.functional.mse_loss(
                    pred_dir_similarity, 
                    true_dir_similarity
                )
            else:
                direction_continuity_loss = torch.tensor(0.0, device=predicted_scanpaths.device)

            # ========== æ›´æ¿€è¿›çš„æŸå¤±æƒé‡ç­–ç•¥ ==========
            # æ ¸å¿ƒæ”¹è¿›ï¼š
            # 1. å¤§å¹…æé«˜Yæ–¹å‘å¤šæ ·æ€§å’Œè¦†ç›–èŒƒå›´æƒé‡
            # 2. è¿›ä¸€æ­¥å¢å¼ºå¹³æ»‘æ€§çº¦æŸ
            # 3. é™ä½é‡æ„æŸå¤±çš„ä¸»å¯¼åœ°ä½

            if epoch <= 80:
                # é˜¶æ®µ1ï¼šå¹³è¡¡é‡æ„å’Œå¤šæ ·æ€§
                coverage_weight = 0.3  # å¤§å¹…æé«˜ï¼ˆä»0.05åˆ°0.3ï¼‰
                diversity_weight = 0.5  # å¤§å¹…æé«˜ï¼ˆä»0.1åˆ°0.5ï¼‰
                center_weight = 0.3  # Yæ–¹å‘ä¸­å¿ƒæƒ©ç½š
                boundary_weight = 0.2  # è¿›ä¸€æ­¥é™ä½
                point_center_weight = 0.0
                smoothness_weight = 1.5  # è¿›ä¸€æ­¥æé«˜ï¼ˆä»1.0åˆ°1.5ï¼‰
                jump_weight = 0.8  # æé«˜ï¼ˆä»0.5åˆ°0.8ï¼‰
                direction_weight = 0.5  # æé«˜ï¼ˆä»0.3åˆ°0.5ï¼‰
                acceleration_weight = 0.5  # æé«˜ï¼ˆä»0.3åˆ°0.5ï¼‰
                direction_continuity_weight = 0.5  # æé«˜ï¼ˆä»0.3åˆ°0.5ï¼‰
            elif epoch <= 150:
                # é˜¶æ®µ2ï¼šè¿›ä¸€æ­¥å¢åŠ å¤šæ ·æ€§çº¦æŸ
                progress = (epoch - 80) / 70.0
                coverage_weight = 0.3 + 0.2 * progress  # 0.3 -> 0.5
                diversity_weight = 0.5 + 0.3 * progress  # 0.5 -> 0.8
                center_weight = 0.3 + 0.2 * progress  # 0.3 -> 0.5
                boundary_weight = 0.2
                point_center_weight = 0.0
                smoothness_weight = 1.5
                jump_weight = 0.8
                direction_weight = 0.5
                acceleration_weight = 0.5
                direction_continuity_weight = 0.5
            else:
                # é˜¶æ®µ3ï¼šæœ€å¤§å¤šæ ·æ€§çº¦æŸ
                coverage_weight = 0.5
                diversity_weight = 0.8
                center_weight = 0.5  # Yæ–¹å‘ä¸­å¿ƒæƒ©ç½š
                boundary_weight = 0.2
                point_center_weight = 0.0
                smoothness_weight = 1.5
                jump_weight = 0.8
                direction_weight = 0.5
                acceleration_weight = 0.5
                direction_continuity_weight = 0.5
            
            # Batchå†…å¤šæ ·æ€§æŸå¤±ï¼šæé«˜æƒé‡
            batch_mean = predicted_scanpaths.mean(dim=0, keepdim=True)  # (1, seq_len, 2)
            batch_diversity = torch.mean((predicted_scanpaths - batch_mean) ** 2)  # æ ‡é‡
            min_batch_diversity = 0.015  # æé«˜é˜ˆå€¼ï¼ˆä»0.01åˆ°0.015ï¼‰
            batch_diversity_loss = torch.mean(((min_batch_diversity - batch_diversity).clamp(min=0.0)) ** 2)

            # æé«˜batchå¤šæ ·æ€§æƒé‡
            if epoch <= 80:
                batch_diversity_weight = 0.2  # æé«˜ï¼ˆä»0.05åˆ°0.2ï¼‰
            elif epoch <= 150:
                progress = (epoch - 80) / 70.0
                batch_diversity_weight = 0.2 + 0.1 * progress  # 0.2 -> 0.3
            else:
                batch_diversity_weight = 0.3  # ä¿æŒ
            
            # ä½¿ç”¨åŠ æƒMSEï¼šå¯¹èµ·å§‹ä½ç½®å’Œå‰å‡ æ­¥ç»™äºˆæ›´é«˜æƒé‡
            # é™ä½æƒé‡å·®å¼‚ï¼Œæ›´å¹³è¡¡å‰åæ­¥éª¤
            position_weights = torch.ones(config.seq_len, device=predicted_scanpaths.device)
            if epoch <= 80:
                # é˜¶æ®µ1ï¼šé€‚åº¦æƒé‡
                position_weights[0] = 2.5  # èµ·å§‹ä½ç½®
                position_weights[1:5] = 1.8  # å‰5æ­¥
                position_weights[5:10] = 1.3  # 5-10æ­¥
            else:
                # é˜¶æ®µ2å’Œ3ï¼šæ›´å¹³è¡¡
                position_weights[0] = 2.0
                position_weights[1:5] = 1.5
                position_weights[5:10] = 1.2
            
            weighted_reconstruction_loss = torch.mean(
                position_weights.unsqueeze(0).unsqueeze(-1) *
                (predicted_scanpaths - true_scanpaths) ** 2
            )

            # æ€»æŸå¤±
            loss = weighted_reconstruction_loss + beta * kl_loss + \
                   coverage_weight * coverage_loss + \
                   diversity_weight * diversity_loss + \
                   center_weight * center_penalty + \
                   boundary_weight * boundary_penalty + \
                   point_center_weight * point_center_penalty + \
                   smoothness_weight * step_length_loss + \
                   jump_weight * jump_penalty + \
                   direction_weight * direction_loss + \
                   acceleration_weight * acceleration_loss + \
                   direction_continuity_weight * direction_continuity_loss + \
                   batch_diversity_weight * batch_diversity_loss

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            # è®¡ç®—ä½ç½®è¯¯å·®
            position_weights_error = torch.ones(config.seq_len, device=predicted_scanpaths.device)
            if epoch <= 80:
                position_weights_error[0] = 2.5
                position_weights_error[1:5] = 1.8
                position_weights_error[5:10] = 1.3
            else:
                position_weights_error[0] = 2.0
                position_weights_error[1:5] = 1.5
                position_weights_error[5:10] = 1.2
            
            # åŠ æƒä½ç½®è¯¯å·®
            weighted_errors = torch.norm(
                predicted_scanpaths - true_scanpaths,
                dim=-1
            ) * position_weights_error.unsqueeze(0)
            position_error = weighted_errors.mean() / position_weights_error.mean()  # å½’ä¸€åŒ–ä»¥ä¿æŒåŸæœ‰å°ºåº¦

            # ç´¯ç§¯æŒ‡æ ‡
            epoch_loss += loss.item()
            epoch_position_error += position_error.item()

            # æ›´æ–°è¿›åº¦æ¡
            if (batch_idx + 1) % config.log_interval == 0:
                avg_loss = epoch_loss / (batch_idx + 1)
                avg_error = epoch_position_error / (batch_idx + 1)
                avg_coverage = coverage_loss.item()
                avg_diversity = diversity_loss.item()
                avg_center = center_penalty.item()
                avg_smooth = step_length_loss.item()
                avg_jump = jump_penalty.item()
                avg_batch_div = batch_diversity_loss.item()
                avg_point_center = point_center_penalty.item()
                avg_acceleration = acceleration_loss.item()
                avg_dir_continuity = direction_continuity_loss.item()
                train_bar.set_postfix({
                    'Loss': f"{avg_loss:.4f}",
                    'PosErr': f"{avg_error:.4f}",
                    'Beta': f"{beta:.4f}",
                    'Cov': f"{avg_coverage:.4f}",
                    'Ctr': f"{avg_center:.4f}",
                    'PCtr': f"{avg_point_center:.4f}",
                    'Smooth': f"{avg_smooth:.4f}",
                    'Jump': f"{avg_jump:.4f}",
                    'Dir': f"{avg_dir_continuity:.4f}",
                    'Acc': f"{avg_acceleration:.4f}",
                })

        # å¹³å‡è®­ç»ƒæŒ‡æ ‡
        num_batches = len(train_loader)
        epoch_loss /= num_batches
        epoch_position_error /= num_batches

        # æ‰“å°è®­ç»ƒç»“æœ
        print(f"\nè®­ç»ƒç»“æœ:")
        print(f"  Loss: {epoch_loss:.4f}")
        print(f"  PositionError: {epoch_position_error:.4f}")

        # éªŒè¯
        if epoch % config.val_interval == 0:
            print(f"\néªŒè¯...")
            model.eval()
            val_loss = 0
            val_position_error = 0

            val_bar = tqdm(test_loader, desc="éªŒè¯")
            with torch.no_grad():
                for batch in val_bar:
                    images = batch['image'].to(config.device)
                    true_scanpaths = batch['scanpath'].to(config.device)

                    # å‰å‘ä¼ æ’­ - éªŒè¯æ¨¡å¼
                    # è®­ç»ƒç›®æ ‡ï¼šéªŒè¯æ¨¡å‹åœ¨æ¨ç†æ—¶çš„çœŸå®æ€§èƒ½
                    # ç­–ç•¥ï¼šä½¿ç”¨æ›´ä½çš„Teacher Forcingï¼Œæ›´æ¥è¿‘æ¨ç†æ—¶çš„0.0
                    # æ˜¾å¼è®¾ç½®enable_early_stop=Falseï¼Œç¡®ä¿è¿”å›3ä¸ªå€¼
                    if epoch <= 50:
                        val_teacher_forcing = 0.2  # é˜¶æ®µ1ï¼šè¾ƒä½Teacher Forcingï¼ˆä»0.3é™åˆ°0.2ï¼‰
                    elif epoch <= 100:
                        val_teacher_forcing = 0.1  # é˜¶æ®µ2ï¼šæ›´ä½Teacher Forcingï¼ˆä»0.2é™åˆ°0.1ï¼‰
                    else:
                        val_teacher_forcing = 0.05  # é˜¶æ®µ3ï¼šéå¸¸ä½Teacher Forcingï¼ˆä»0.1é™åˆ°0.05ï¼‰
                    result = model(images, gt_scanpaths=true_scanpaths, teacher_forcing_ratio=val_teacher_forcing, enable_early_stop=False)
                    # å®‰å…¨è§£åŒ…ï¼šæ— è®ºè¿”å›3ä¸ªè¿˜æ˜¯5ä¸ªå€¼ï¼Œéƒ½åªå–å‰3ä¸ª
                    predicted_scanpaths = result[0]
                    mus = result[1]
                    logvars = result[2]

                    # è®¡ç®—VAEæŸå¤±å‡½æ•°ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰
                    # 1. é‡æ„æŸå¤±ï¼ˆç”¨äºç›‘æ§ï¼‰
                    reconstruction_loss = nn.functional.mse_loss(predicted_scanpaths, true_scanpaths)

                    # 2. KLæ•£åº¦æ­£åˆ™åŒ–
                    kl_loss = -0.5 * torch.sum(1 + logvars - mus.pow(2) - logvars.exp())
                    kl_loss = kl_loss / (mus.size(0) * mus.size(1))

                    # 3. Beta-VAEæƒé‡ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰
                    beta = min(0.01, 0.005 * (1.01 ** (epoch - 1)))

                    # 4. è¦†ç›–èŒƒå›´æŸå¤±ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼Œä½¿ç”¨æ”¹è¿›åçš„ç‰ˆæœ¬ï¼‰
                    pred_min = predicted_scanpaths.min(dim=1)[0]
                    pred_max = predicted_scanpaths.max(dim=1)[0]
                    pred_range = pred_max - pred_min
                    coverage_loss = torch.mean(((0.3 - pred_range).clamp(min=0.0)) ** 2) * 0.5
                    
                    # 5. ä½ç½®å¤šæ ·æ€§æŸå¤±ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼Œä½¿ç”¨æ”¹è¿›åçš„ç‰ˆæœ¬ï¼‰
                    pred_mean = predicted_scanpaths.mean(dim=1)
                    pred_var = ((predicted_scanpaths - pred_mean.unsqueeze(1)) ** 2).mean(dim=1)
                    min_var = 0.015
                    diversity_loss = torch.mean(((min_var - pred_var).clamp(min=0.0)) ** 2) * 0.5
                    
                    # 6. è¾¹ç•Œçº¦æŸï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
                    boundary_min = 0.05
                    boundary_max = 0.95
                    below_boundary = (predicted_scanpaths < boundary_min).float()
                    above_boundary = (predicted_scanpaths > boundary_max).float()
                    boundary_penalty = torch.mean(
                        below_boundary * (boundary_min - predicted_scanpaths) ** 2 +
                        above_boundary * (predicted_scanpaths - boundary_max) ** 2
                    ) * 10.0
                    
                    # 7. æ¸©å’Œçš„ä¸­å¿ƒèšé›†æƒ©ç½šï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
                    mean_center_dist = torch.mean((pred_mean - 0.5) ** 2, dim=-1)
                    very_close_to_center = (mean_center_dist < 0.01).float()
                    center_penalty = torch.mean(very_close_to_center * (0.01 - mean_center_dist) * 5.0)
                    point_center_penalty = torch.tensor(0.0, device=predicted_scanpaths.device)
                    
                    # 8. è½¨è¿¹å¹³æ»‘æ€§æŸå¤±ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼ŒåŒ…æ‹¬æ–°å¢çš„åŠ é€Ÿåº¦å’Œæ–¹å‘è¿ç»­æ€§ï¼‰
                    pred_diffs = predicted_scanpaths[:, 1:] - predicted_scanpaths[:, :-1]
                    true_diffs = true_scanpaths[:, 1:] - true_scanpaths[:, :-1]
                    pred_step_lengths = torch.norm(pred_diffs, p=2, dim=-1)
                    true_step_lengths = torch.norm(true_diffs, p=2, dim=-1)
                    step_length_loss = nn.functional.mse_loss(pred_step_lengths, true_step_lengths)
                    large_jumps = (pred_step_lengths - 0.20).clamp(min=0.0)
                    jump_penalty = torch.mean(large_jumps ** 2)
                    
                    if pred_diffs.shape[1] > 1:
                        pred_directions = pred_diffs / (pred_step_lengths.unsqueeze(-1) + 1e-8)
                        true_directions = true_diffs / (true_step_lengths.unsqueeze(-1) + 1e-8)
                        pred_dir_diffs = pred_directions[:, 1:] - pred_directions[:, :-1]
                        true_dir_diffs = true_directions[:, 1:] - true_directions[:, :-1]
                        direction_loss = nn.functional.mse_loss(
                            torch.norm(pred_dir_diffs, p=2, dim=-1),
                            torch.norm(true_dir_diffs, p=2, dim=-1)
                        )
                    else:
                        direction_loss = torch.tensor(0.0, device=predicted_scanpaths.device)
                    
                    # åŠ é€Ÿåº¦çº¦æŸï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
                    if pred_step_lengths.shape[1] > 1:
                        pred_acceleration = pred_step_lengths[:, 1:] - pred_step_lengths[:, :-1]
                        true_acceleration = true_step_lengths[:, 1:] - true_step_lengths[:, :-1]
                        acceleration_loss = nn.functional.mse_loss(pred_acceleration, true_acceleration)
                    else:
                        acceleration_loss = torch.tensor(0.0, device=predicted_scanpaths.device)
                    
                    # æ–¹å‘è¿ç»­æ€§ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
                    if pred_directions.shape[1] > 0:
                        pred_dir_similarity = F.cosine_similarity(
                            pred_directions[:, :-1], 
                            pred_directions[:, 1:], 
                            dim=-1
                        )
                        true_dir_similarity = F.cosine_similarity(
                            true_directions[:, :-1], 
                            true_directions[:, 1:], 
                            dim=-1
                        )
                        direction_continuity_loss = nn.functional.mse_loss(
                            pred_dir_similarity, 
                            true_dir_similarity
                        )
                    else:
                        direction_continuity_loss = torch.tensor(0.0, device=predicted_scanpaths.device)
                    
                    # 9. Batchå†…å¤šæ ·æ€§æŸå¤±ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼Œä½¿ç”¨æ”¹è¿›åçš„ç‰ˆæœ¬ï¼‰
                    batch_mean = predicted_scanpaths.mean(dim=0, keepdim=True)
                    batch_diversity = torch.mean((predicted_scanpaths - batch_mean) ** 2)
                    min_batch_diversity = 0.01
                    batch_diversity_loss = torch.mean(((min_batch_diversity - batch_diversity).clamp(min=0.0)) ** 2)
                    
                    # 9. åŠ æƒMSEæŸå¤±ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼Œåˆ†é˜¶æ®µï¼‰
                    position_weights = torch.ones(config.seq_len, device=predicted_scanpaths.device)
                    if epoch <= 80:
                        position_weights[0] = 3.0
                        position_weights[1:5] = 2.0
                        position_weights[5:10] = 1.5
                    else:
                        position_weights[0] = 2.0
                        position_weights[1:5] = 1.5
                        position_weights[5:10] = 1.2
                    weighted_reconstruction_loss = torch.mean(
                        position_weights.unsqueeze(0).unsqueeze(-1) * 
                        (predicted_scanpaths - true_scanpaths) ** 2
                    )

                    # æ€»æŸå¤±ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼Œä½¿ç”¨æ”¹è¿›åçš„ç‰ˆæœ¬ï¼‰
                    if epoch <= 80:
                        coverage_weight = 0.02
                        diversity_weight = 0.01
                        center_weight = 0.01
                        boundary_weight = 0.5
                        point_center_weight = 0.0
                        smoothness_weight = 0.15
                        jump_weight = 0.05
                        direction_weight = 0.05
                        acceleration_weight = 0.05
                        direction_continuity_weight = 0.05
                        batch_diversity_weight = 0.0
                    elif epoch <= 150:
                        progress = (epoch - 80) / 70.0
                        coverage_weight = 0.02 + 0.03 * progress  # ä»0.02å¢åŠ åˆ°0.05
                        diversity_weight = 0.01 + 0.02 * progress  # ä»0.01å¢åŠ åˆ°0.03
                        center_weight = 0.01 + 0.02 * progress  # ä»0.01å¢åŠ åˆ°0.03
                        boundary_weight = 0.5 - 0.2 * progress  # ä»0.5é™ä½åˆ°0.3
                        point_center_weight = 0.0
                        smoothness_weight = 0.15 + 0.1 * progress
                        jump_weight = 0.05 + 0.05 * progress
                        direction_weight = 0.05 + 0.05 * progress
                        acceleration_weight = 0.05 + 0.05 * progress
                        direction_continuity_weight = 0.05 + 0.05 * progress
                        batch_diversity_weight = 0.01 * progress  # ä»0å¢åŠ åˆ°0.01
                    else:
                        coverage_weight = 0.05
                        diversity_weight = 0.03
                        center_weight = 0.03
                        boundary_weight = 0.3
                        point_center_weight = 0.0
                        smoothness_weight = 0.25
                        jump_weight = 0.1
                        direction_weight = 0.1
                        acceleration_weight = 0.1
                        direction_continuity_weight = 0.1
                        batch_diversity_weight = 0.01
                    
                    loss = weighted_reconstruction_loss + beta * kl_loss + \
                           coverage_weight * coverage_loss + \
                           diversity_weight * diversity_loss + \
                           center_weight * center_penalty + \
                           boundary_weight * boundary_penalty + \
                           point_center_weight * point_center_penalty + \
                           smoothness_weight * step_length_loss + \
                           jump_weight * jump_penalty + \
                           direction_weight * direction_loss + \
                           acceleration_weight * acceleration_loss + \
                           direction_continuity_weight * direction_continuity_loss + \
                           batch_diversity_weight * batch_diversity_loss

                    # è®¡ç®—ä½ç½®è¯¯å·®ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼Œä½¿ç”¨åŠ æƒè¯¯å·®ï¼Œåˆ†é˜¶æ®µï¼‰
                    position_weights_error = torch.ones(config.seq_len, device=predicted_scanpaths.device)
                    if epoch <= 80:
                        position_weights_error[0] = 3.0  # ä¸è®­ç»ƒæ—¶ä¸€è‡´
                        position_weights_error[1:5] = 2.0
                        position_weights_error[5:10] = 1.5
                    else:
                        position_weights_error[0] = 2.0
                        position_weights_error[1:5] = 1.5
                        position_weights_error[5:10] = 1.2
                    
                    weighted_errors = torch.norm(
                        predicted_scanpaths - true_scanpaths,
                        dim=-1
                    ) * position_weights_error.unsqueeze(0)
                    position_error = weighted_errors.mean() / position_weights_error.mean()

                    val_loss += loss.item()
                    val_position_error += position_error.item()

            # å¹³å‡éªŒè¯æŒ‡æ ‡
            num_val_batches = len(test_loader)
            val_loss /= num_val_batches
            val_position_error /= num_val_batches

            print(f"\néªŒè¯ç»“æœ:")
            print(f"  Loss: {val_loss:.4f}")
            print(f"  PositionError: {val_position_error:.4f}")

            # å­¦ä¹ ç‡è°ƒåº¦ - ExponentialLRåœ¨æ¯ä¸ªepochåè‡ªåŠ¨è¡°å‡
            current_lr = optimizer.param_groups[0]['lr']
            print(f"  Learning Rate: {current_lr:.6f}")

            # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼šä¼˜å…ˆåŸºäºä½ç½®è¯¯å·®ï¼Œä¹Ÿè€ƒè™‘æŸå¤±
            save_model = False
            if val_position_error < best_val_position_error:
                best_val_position_error = val_position_error
                save_model = True
                patience_counter = 0  # é‡ç½®æ—©åœè®¡æ•°å™¨ï¼ˆåŸºäºä½ç½®è¯¯å·®ï¼‰
                print(f"  âœ… éªŒè¯ä½ç½®è¯¯å·®æ”¹å–„: {val_position_error:.4f} (æ–°æœ€ä½³)")
            
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                if not save_model:  # å¦‚æœä½ç½®è¯¯å·®æ²¡æ”¹å–„ä½†æŸå¤±æ”¹å–„äº†ï¼Œä¹Ÿä¿å­˜
                    save_model = True
            
            if save_model:
                best_path = os.path.join(config.checkpoint_dir, 'best_model.pth')
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'best_loss': best_val_loss,
                    'best_position_error': best_val_position_error,
                }, best_path)
                print(f"  ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")
                print(f"     æœ€ä½³éªŒè¯ä½ç½®è¯¯å·®: {best_val_position_error:.4f}")
                print(f"     æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
            else:
                patience_counter += 1
                print(f"  âš ï¸ éªŒè¯ä½ç½®è¯¯å·®æœªæ”¹å–„ ({patience_counter}/{early_stopping_patience})")
                print(f"     å½“å‰: {val_position_error:.4f}, æœ€ä½³: {best_val_position_error:.4f}")

            # æ—©åœæ£€æŸ¥ï¼šåŸºäºä½ç½®è¯¯å·®
            if patience_counter >= early_stopping_patience:
                print(f"\nâ¹ï¸ æ—©åœè§¦å‘ï¼éªŒè¯ä½ç½®è¯¯å·®å·²ç»{early_stopping_patience}ä¸ªepochæ²¡æœ‰æ”¹å–„")
                print(f"æœ€ä½³éªŒè¯ä½ç½®è¯¯å·®: {best_val_position_error:.4f}")
                print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
                break

        # ä¿å­˜æ£€æŸ¥ç‚¹
        if epoch % config.save_interval == 0:
            checkpoint_path = os.path.join(
                config.checkpoint_dir,
                f'checkpoint_epoch_{epoch}.pth'
            )
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
            }, checkpoint_path)
            print(f"  ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")

        # è·å–å½“å‰å­¦ä¹ ç‡
        current_lr = optimizer.param_groups[0]['lr']

        # è®°å½•æ—¥å¿—
        epoch_log = {
            'epoch': epoch,
            'learning_rate': current_lr,
            'train': {
                'loss': epoch_loss,
                'position_error': epoch_position_error,
            },
        }
        if epoch % config.val_interval == 0:
            epoch_log['val'] = {
                'loss': val_loss,
                'position_error': val_position_error,
            }

        training_log['epochs'].append(epoch_log)

        # ä¿å­˜è®­ç»ƒæ—¥å¿—
        log_path = os.path.join(config.log_dir, 'training_log.json')
        with open(log_path, 'w') as f:
            json.dump(training_log, f, indent=2)

        # å­¦ä¹ ç‡è¡°å‡ï¼ˆæ¯ä¸ªepochç»“æŸåï¼‰
        scheduler.step()

    print("\nè®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_loss:.4f}")
    print(f"\nä¸‹ä¸€æ­¥ï¼šä½¿ç”¨ visualize_mamba_agent.py å¯è§†åŒ–ç»“æœ")


if __name__ == '__main__':
    train()
