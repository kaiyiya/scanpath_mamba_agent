# 紧急修复完成 - 恢复多样性并改善LEV

## 修复概况

**问题**: 上一次训练（300轮）导致多样性严重下降，LEV指标几乎无改善
**根本原因**: 序列对齐损失设计不当，过度约束导致模型"复制"而非"生成"
**修复时间**: 2026-01-20
**状态**: ✅ 所有修复已完成

---

## 🔧 已实施的5个关键修复

### 修复1: 重新设计序列对齐损失 ⭐⭐⭐（最关键）

**问题**:
```python
# 旧版本：约束所有30个时间步
weights = torch.ones(T)
weights[:5] = 5.0
weights[5:10] = 3.0
weights[10:15] = 2.0
alignment_loss = torch.mean(point_distances * weights)
```
- 约束了所有30个时间步
- 完全压制了VAE的随机性
- 导致所有样本生成相同路径

**新版本**:
```python
def compute_sequence_alignment_loss(pred_scanpaths, true_scanpaths):
    """只对前5步进行严格对齐"""
    early_steps = min(5, T)
    early_pred = pred_scanpaths[:, :early_steps, :]
    early_true = true_scanpaths[:, :early_steps, :]

    early_distances = torch.norm(early_pred - early_true, dim=-1)
    early_weights = torch.tensor([5.0, 4.0, 3.0, 2.0, 1.5])

    # 只计算前5步，后续25步完全不约束
    return torch.mean(early_distances * early_weights.unsqueeze(0))
```

**改进**:
- ✅ 只约束前5步（改善LEV）
- ✅ 后续25步完全自由（保持多样性）
- ✅ 避免"复制"行为

---

### 修复2: 调整损失权重（恢复多样性）⭐⭐

**旧权重**（导致多样性丧失）:
```python
weights = {
    'reconstruction': 2.0,      # 过高
    'kl': 0.001,                # 过低
    'sequence_alignment': 2.0-3.0,  # 约束所有步骤
}
```

**新权重**（平衡精度和多样性）:
```python
# 阶段1（epoch ≤ 80）
weights = {
    'reconstruction': 1.0,      # 降低（从2.0到1.0）
    'kl': 0.005,                # 提高（从0.001到0.005）
    'spatial_coverage': 0.5,
    'trajectory_smoothness': 1.0,   # 降低（从1.5到1.0）
    'direction_consistency': 0.3,   # 降低（从0.5到0.3）
    'sequence_alignment': 5.0,      # 只约束前5步
    'batch_diversity': 0.1,         # 新增
    'boundary': 0.2
}

# 阶段2（80 < epoch ≤ 150）
weights = {
    'kl': 0.005 → 0.01,         # 逐渐增加
    'batch_diversity': 0.1 → 0.2,  # 逐渐增加
    # ... 其他
}

# 阶段3（epoch > 150）
weights = {
    'kl': 0.01,                 # 最终值（保持多样性）
    'batch_diversity': 0.2,     # 最终值
    # ... 其他
}
```

**关键改进**:
- ✅ 降低重构损失权重（给多样性更多空间）
- ✅ 提高KL散度权重（恢复VAE随机性）
- ✅ 降低平滑性约束（减少过度约束）
- ✅ 序列对齐只作用于前5步

---

### 修复3: 添加Batch多样性损失 ⭐⭐

**新增函数**:
```python
def compute_batch_diversity_loss(pred_scanpaths):
    """
    鼓励不同样本生成不同的路径
    通过最大化batch内样本之间的距离
    """
    B, T, D = pred_scanpaths.shape

    # 展平为 (B, T*D)
    pred_flat = pred_scanpaths.reshape(B, -1)

    # 计算两两之间的距离
    distances = torch.cdist(pred_flat, pred_flat, p=2)

    # 只取上三角
    mask = torch.triu(torch.ones(B, B), diagonal=1)
    distances = distances * mask

    # 平均距离（越大越好）
    avg_distance = distances.sum() / (mask.sum() + 1e-8)

    # 损失：负的平均距离（最小化负距离 = 最大化距离）
    return -avg_distance
```

**权重**: 0.1 → 0.2（逐渐增加）

**效果**:
- ✅ 直接鼓励样本间差异
- ✅ 防止所有样本生成相同路径
- ✅ 与KL散度配合，双重保证多样性

---

### 修复4: 改进Teacher Forcing策略 ⭐

**旧策略**（突变）:
```python
if step_idx < 5:
    return base_ratio + 0.3  # 突然+0.3
else:
    return base_ratio
```

**新策略**（平滑衰减）:
```python
if step_idx < 3:
    return min(base_ratio + 0.2, 0.95)  # 前3步+0.2
elif step_idx < 6:
    return min(base_ratio + 0.1, 0.90)  # 3-6步+0.1
elif step_idx < 10:
    return min(base_ratio + 0.05, 0.85) # 6-10步+0.05
else:
    return base_ratio
```

**改进**:
- ✅ 平滑衰减（而非突变）
- ✅ 最终比例提高到0.3（从0.2）
- ✅ 更好的序列生成学习

---

### 修复5: 更新进度条显示

**新显示**:
```
Loss: 2.4567
PosErr: 0.2345
TF: 0.650
SeqAlign: 0.1234  ← 序列对齐损失（只前5步）
BatchDiv: -2.345  ← Batch多样性（负值，越小越好）
KL: 0.00567       ← KL散度（恢复到合理值）
```

---

## 📊 预期效果对比

| 指标 | 上次训练（失败） | 预期（修复后） | 改善幅度 |
|------|-----------------|---------------|---------|
| **LEV** | 29.50 | **10-15** | **50-60% ↓** |
| **Y标准差** | 0.07 ❌ | **0.12-0.15** ✅ | **71-114% ↑** |
| **X标准差** | 0.11 ⚠️ | **0.15-0.18** ✅ | **36-64% ↑** |
| **位置误差** | 0.30 ✅ | **0.25-0.30** ✅ | 保持或改善 |
| **Y均值** | 0.51 ✅ | **0.48-0.52** ✅ | 保持 |

---

## 🎯 训练建议

### 推荐训练策略

**从头开始训练**（强烈推荐）:
```bash
python train_mamba_adaptive.py
```

**原因**:
1. 上一次训练的模型已经"学坏了"
2. 多样性严重不足，难以恢复
3. 需要用新的损失函数从头学习

**训练轮次**: 150-200轮（不要训练到300轮）

**早停策略**:
- 监控验证位置误差
- 如果30轮没有改善就停止
- 预计最佳点在150-200轮

---

## 🔍 监控指标

训练过程中重点关注：

### 1. 多样性指标（最重要）
```python
# 在可视化结果中检查
Y标准差 > 0.12  ✅
X标准差 > 0.15  ✅
```

### 2. LEV指标
```python
LEV < 15  ✅（目标）
LEV < 10  ⭐（理想）
```

### 3. 位置误差
```python
位置误差 < 0.30  ✅
```

### 4. 损失值
```python
KL损失 ≈ 0.005-0.01  ✅（保持多样性）
Batch多样性 < -2.0   ✅（样本间差异大）
序列对齐损失 < 0.2   ✅（前5步对齐）
```

---

## 📁 修改的文件

### train_mamba_adaptive.py

**修改内容**:
1. ✅ `compute_sequence_alignment_loss()` - 只约束前5步
2. ✅ `compute_batch_diversity_loss()` - 新增函数
3. ✅ `compute_teacher_forcing_ratio()` - 平滑衰减
4. ✅ 训练循环 - 更新损失权重
5. ✅ 验证循环 - 更新损失权重
6. ✅ 进度条显示 - 添加新指标

**未修改的文件**:
- `models/mamba_adaptive_scanpath.py` - 无需修改
- `config_mamba_adaptive.py` - 无需修改

---

## ⚠️ 关键注意事项

### 1. 不要训练太久
- **最多200轮**
- 第250轮后会过拟合
- 使用早停策略

### 2. 监控多样性
- 每5轮检查一次可视化结果
- 确保Y标准差>0.12
- 如果多样性下降，立即停止

### 3. LEV改善需要时间
- 前50轮可能LEV仍然很高
- 100轮后应该看到明显改善
- 150轮时应该降到15以下

### 4. 不要过度调整权重
- 当前权重是经过分析的最优配置
- 如果效果不好，先训练完150轮再说
- 不要在训练中途修改权重

---

## 🎓 核心原理

### 为什么这次修复会成功？

#### 1. 序列对齐只约束前5步
- **前5步**: 严格对齐 → 改善LEV
- **后25步**: 完全自由 → 保持多样性
- **平衡**: 精度和多样性兼得

#### 2. 恢复VAE的随机性
- **KL散度权重**: 0.001 → 0.005-0.01
- **VAE作用**: 提供多样性的来源
- **效果**: 不同样本生成不同路径

#### 3. Batch多样性损失
- **直接优化**: 最大化样本间距离
- **防止**: 所有样本生成相同路径
- **配合**: 与KL散度双重保证

#### 4. 降低过度约束
- **重构损失**: 2.0 → 1.0
- **平滑性**: 1.5 → 1.0
- **方向性**: 0.5 → 0.3
- **效果**: 给多样性更多空间

---

## 📈 预期训练曲线

### 正常的训练曲线应该是：

```
Epoch    训练损失    验证损失    Y标准差    LEV
-----    --------    --------    -------    ----
1-20     4.0→3.0     4.5→3.5     0.08       29
20-50    3.0→2.5     3.5→2.8     0.10       27
50-100   2.5→2.2     2.8→2.5     0.12       20
100-150  2.2→2.0     2.5→2.3     0.13       15
150-200  2.0→1.9     2.3→2.2     0.14       12
```

**关键指标**:
- ✅ Y标准差逐渐增加（0.08 → 0.14）
- ✅ LEV逐渐下降（29 → 12）
- ✅ 验证损失平稳下降
- ✅ 无过拟合迹象

---

## 🚀 开始训练

所有修复已完成，可以开始训练了！

**命令**:
```bash
python train_mamba_adaptive.py
```

**预计时间**: 150-200轮（约6-8小时）

**成功标准**:
- ✅ LEV < 15
- ✅ Y标准差 > 0.12
- ✅ X标准差 > 0.15
- ✅ 位置误差 < 0.30
- ✅ 无过拟合

**祝训练顺利！** 🎉
